{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d7d466",
   "metadata": {},
   "source": [
    "# Mining Transformer self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ae430",
   "metadata": {},
   "source": [
    "This notebook implements the training of a DSDM instance (located in folder [src/lib/memory/DSDM.py](https://github.com/dfichiu/ba-thesis/blob/master/src/lib/memory/DSDM.py)) with subsequences constructed by passing each sentence through the pre-traianed [BERT base uncase](https://huggingface.co/bert-base-uncased) and mining the resulting self-attention matrices. Inference is also performed for the in-set inference sentences.\n",
    "\n",
    "The experiment currently run trains on 20 articles (10 are the inference articles) on the full attention landscape (144 heads - 12 layers w/ 12 heads/layer). The subsequences are first sorted by length and then by chunk score, with a single subsequence per head being committed to memory. Stop words are removed during training (See [Training II](###Training-II): Subsequence construction parameters), but they are kept in during inference (they act as noise.) The preprocessing of the sentence during inference is implemented in the `infer` function from the [src/lib/utils/inference.py](https://github.com/dfichiu/ba-thesis/blob/0524e5598786147aefad596641bff0c0a061cd1f/src/lib/utils/inference.py#L190) module.\n",
    "\n",
    "\n",
    "For longer training, considering using the training script [/src/experiments/train_memory.py](https://github.com/dfichiu/ba-thesis/blob/master/src/experiments/train_memory.py).\n",
    "\n",
    "For a description of the parameters that can be set during training and inference, please refer to the respective sections:\n",
    "- [Training I](###Training-I): DSDM parameters\n",
    "- [Training II](###Training-II): Subsequence construction parameters\n",
    "- [Inference](#Inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e1a382-932e-4312-b5d6-f7b2bbe4518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set path for imports. ###\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ffe562-2195-4dbb-af53-60350a4b3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, Markdown as md\n",
    "import itertools\n",
    "\n",
    "from lib.memory import DSDM\n",
    "from lib.utils import cleanup, configs, inference, learning, preprocess, utils \n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Package options ###\n",
    "## Torch\n",
    "# Disable gradients.\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d388cf6-77de-40bd-83b6-c5db77acbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils ###\n",
    "def plot_heatmap(x: np.array, labels: np.array) -> None:\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(\n",
    "        x,\n",
    "        linewidth=0.5,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "    )\n",
    "    plt.title(f'Self-attention matrix: layer {layer}, head {head}', fontsize=15)\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def average_out_and_remove_rows(\n",
    "    t: torch.tensor,\n",
    "    averages_idx: list,\n",
    "    remove_idx: np.array\n",
    ") -> torch.tensor:\n",
    "    for average_idx in averages_idx:  # The nested lists can have different dimensions.\n",
    "        # Replace the attention scores of the first token with the average of the token attention scores.\n",
    "        t[min(average_idx)] = torch.mean(t[average_idx], dim=0, keepdim=True)\n",
    "    return t[~remove_idx]\n",
    "\n",
    "\n",
    "def preprocess_attention_scores(\n",
    "    attention_scores: torch.tensor,\n",
    "    averages_idx: list,\n",
    "    remove_idx: np.array\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess self-attention matrix.\n",
    "    \n",
    "    Average out rows associated with subwords to create entries of reconstructed\n",
    "    words. Remove punctuation, stop words, and subwords. Apply same procedure to columns by\n",
    "    transposing the matrix.\n",
    "    \"\"\"\n",
    "    # Remove entries from rows.\n",
    "    attention_scores = average_out_and_remove_rows(attention_scores, averages_idx, remove_idx)\n",
    "    # Transpose matrix.\n",
    "    attention_scores = attention_scores.transpose(0, 1)\n",
    "    # Remove entries from columns.\n",
    "    attention_scores = average_out_and_remove_rows(attention_scores, averages_idx, remove_idx)\n",
    "    # Transpose matrix.\n",
    "    return attention_scores.transpose(0, 1)\n",
    "        \n",
    "    \n",
    "\n",
    "def backward_pass(G, current_node, left_edge, right_edge, sequence, mean):\n",
    "    in_nodes = np.array([edge[0] for edge in list(G.in_edges(current_node))])\n",
    "    in_nodes = in_nodes[(in_nodes > left_edge) & (in_nodes < current_node)]\n",
    "    for node in in_nodes:\n",
    "        sequence[node] = 1\n",
    "        sequences.append(sequence)\n",
    "        mean += G[node][current_node]['weight']\n",
    "        means.append(round(mean / (sum(sequence) - 1), 2))\n",
    "        backward_pass(G, node, left_edge, node, sequence.copy(), mean)\n",
    "        forward_pass(G, node, left_edge, current_node, sequence.copy(), mean)\n",
    "        \n",
    "    return\n",
    "    \n",
    "    \n",
    "def forward_pass(G, current_node, left_edge, right_edge, sequence, mean):\n",
    "    out_nodes = np.array([edge[1] for edge in list(G.out_edges(current_node))])\n",
    "    out_nodes = out_nodes[(out_nodes > current_node) & (out_nodes < right_edge)]\n",
    "    for node in out_nodes:\n",
    "        sequence[node] = 1\n",
    "        mean += G[current_node][node]['weight']\n",
    "        sequences.append(sequence)\n",
    "        means.append(round(mean / (sum(sequence) - 1), 2))\n",
    "        backward_pass(G, node, current_node, node, sequence.copy(), mean)\n",
    "        forward_pass(G, node, node, right_edge, sequence.copy(), mean)\n",
    "            \n",
    "    return\n",
    "    \n",
    "\n",
    "def construct_sequences(G: nx.DiGraph, n_tokens):\n",
    "    \"\"\"Construct subsequences from weighted directed graph.\"\"\"\n",
    "    for node in G.nodes():\n",
    "        sequence = np.zeros(n_tokens)\n",
    "        mean = 0\n",
    "        sequence[node] = 1\n",
    "        #sequences.append(sequence) # Do not allow for 1-token sequences.\n",
    "        forward_pass(G, node, node, n_tokens, sequence.copy(), mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffef961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(cleanup, memory):\n",
    "    \"\"\"Save codebook and memory to file.\"\"\"\n",
    "    now = str(datetime.now()).replace(':', \"-\").replace('.', '-')\n",
    "    \n",
    "    if not os.path.exists('memories/method2'):\n",
    "        os.makedirs('memories/method2')\n",
    "    if not os.path.exists('cleanups/method2'):\n",
    "        os.makedirs('cleanups/method2')\n",
    "        \n",
    "    with open(f'memories/method2/memory_{now}.pkl', 'wb') as outp:\n",
    "        pickle.dump(memory, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'cleanups/method2/cleanup_{now}.pkl', 'wb') as outp:\n",
    "        pickle.dump(cleanup, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f8bd36-2a2e-472c-b5af-ffe9ed6cc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/nfs/data/projects/daniela/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689c656c38034a92b765f7560fd05251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Wikipedia dataset.\n",
    "# TODO: Split between server and local.\n",
    "# wiki_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.en\")['train']\n",
    "wiki_dataset = datasets.load_dataset(\n",
    "    \"wikipedia\",\n",
    "    \"20220301.en\",\n",
    "    cache_dir=\"/nfs/data/projects/daniela\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6025b42d-acd1-4bc3-9a13-9069a0747f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Using seed: 41"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "utils.fix_seed(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d755aa5",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Training I\n",
    "\n",
    "<ins>**DSDM parameters**</ins> \n",
    "- `address_size`\n",
    "- `ema_time_period`, `learning_rate_update`, `normalize` - These parameters shouldn't change, as their values influence whether DSDM aggregates during saving or not;\n",
    "- `as_threshold`\n",
    "- `temperature`\n",
    "- `prune_mode`\n",
    "- `max_size_address_space`\n",
    "\n",
    "- `safeguard_bins`\n",
    "- `bin_score_threshold_type`\n",
    "- `bin_score_threshold`\n",
    " \n",
    "- `safeguard_chunks`\n",
    "- `chunk_score_threshold`\n",
    "\n",
    "For a documentation of the DSDM parameters, please refer to the DSDM class, located in the folder [src/lib/memory/DSDM.py](https://github.com/dfichiu/ba-thesis/blob/master/src/lib/memory/DSDM.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d79c11-11fa-4160-ae31-0b5d9978f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DSDM parameters ###\n",
    "# These parameters shouldn't change.\n",
    "address_size = 1000\n",
    "ema_time_period = 100000\n",
    "learning_rate_update = 0\n",
    "\n",
    "normalize = False \n",
    "\n",
    "# Attention score threshold\n",
    "as_threshold = 0.5\n",
    "\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "# Pruning parameters\n",
    "prune_mode = None\n",
    "max_size_address_space = 10\n",
    "\n",
    "safeguard_bins = True\n",
    "bin_score_threshold_type = 'static'\n",
    "bin_score_threshold = 1e-8\n",
    " \n",
    "safeguard_chunks = True\n",
    "chunk_score_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f277d1-3913-4c25-8b78-e6526c0ea5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize codebook, i.e., class that saves token - atomic hypervector associations.\n",
    "cleanup = cleanup.Cleanup(address_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69f820f-3847-458d-a11a-875f02f02552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT base uncased and Wordpiece tokenizer.\n",
    "model_name = \"bert-base-uncased\"  # Has 12 layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# The BERT model can process texts of the maximal length of 512 tokens.\n",
    "MAXIMUM_SEQUENCE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dad53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DSDM object.\n",
    "memory = DSDM.DSDM(\n",
    "    address_size=address_size,\n",
    "    ema_time_period=ema_time_period,\n",
    "    learning_rate_update=learning_rate_update,\n",
    "    temperature=temperature,\n",
    "    normalize=normalize,\n",
    "    prune_mode=prune_mode,\n",
    "    max_size_address_space=max_size_address_space,\n",
    "    safeguard_bins=safeguard_bins,\n",
    "    bin_score_threshold_type=bin_score_threshold_type,\n",
    "    bin_score_threshold=bin_score_threshold,\n",
    "    safeguard_chunks=safeguard_chunks,\n",
    "    chunk_score_threshold=chunk_score_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce24920",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10 # Parameter: Number of train articles\n",
    "\n",
    "train_idx = np.random.randint(0, len(wiki_dataset) - 1000, size=1000000)\n",
    "# Select train articles.\n",
    "train_idx = train_idx[:train_size]\n",
    "# Manually add the articles from which the in-set inference sentences were selected.\n",
    "train_idx = np.append(np.array([6458629, 6458633, 6458645, 6458648, 6458659, 6458664, 6458665,\n",
    "   6458667, 6458668, 6458573]), train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebe1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Not used ###\n",
    "# Global duplicated addresses counter.\n",
    "dups_found = 0\n",
    "\n",
    "def remove_duplicates(memory):\n",
    "    \"\"\"Remove duplicate addresses from a DSDM object.\n",
    "    \n",
    "    Given a DSDM object, for each address, remove address that have a (cosine) similarity\n",
    "    higer than 0.95 to it.\n",
    "    \n",
    "    Implemented by a global keep mask that is updated for each address using 'and.'\n",
    "    \"\"\"\n",
    "    global dups_found\n",
    "    global_keep_mask = torch.tensor([True] * len(memory.addresses)).to(device)\n",
    "    \n",
    "    for idx, address in enumerate(memory.addresses):\n",
    "        if global_keep_mask[idx].item():\n",
    "            cos = torch.nn.CosineSimilarity()\n",
    "            keep_mask = cos(memory.addresses, address) < 0.95\n",
    "            # Keep current address.\n",
    "            keep_mask[idx] = True\n",
    "            global_keep_mask &= keep_mask\n",
    "\n",
    "    if global_keep_mask.sum().item() > 0:\n",
    "        dups_found += 1\n",
    "        # Remove similar addresses.\n",
    "        memory.addresses = memory.addresses[global_keep_mask]\n",
    "        # Remove bins.\n",
    "        memory.bins = memory.bins[global_keep_mask]\n",
    "        # Remove chunk scores.\n",
    "        memory.chunk_scores = memory.chunk_scores[global_keep_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c9e2c",
   "metadata": {},
   "source": [
    "### Training II\n",
    "\n",
    "<ins>**Subsequence construction parameters**</ins>\n",
    "    \n",
    "Regarding subsequence construction, the following parameters/settings can be adjusted in the below cell:\n",
    "- **Stop words:** Either left in or removed. Currently, they are removed;\n",
    "- **Subsequence sorting:** The generated subsequences can be arranged in two ways: they can be initially sorted by chunk score and then by length (in descending order), or conversely. Currently, the subsequences are first sorted by length and then by chunk score;\n",
    "- **Subsequence number:** The number of subsequences to save to memory after sorting; Currently, the number is set to 1.\n",
    "- **Self-attention layer:** Which encoder layers to costruct subsequences from.\n",
    "\n",
    "The places in code where the above setting can be set are marked by a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f395fae-f8ce-490b-80ac-a1675280b29f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:11<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "for pos, i in enumerate(tqdm(train_idx)):\n",
    "    # Add article number to DSDM for statistics.\n",
    "    memory.add_wiki_article(int(i))\n",
    "    # Get text from article.\n",
    "    text = wiki_dataset[int(i)]['text']\n",
    "    \n",
    "    # Split text into sentences.\n",
    "    sentences = preprocess.split_text_into_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        if inputs['input_ids'].shape[1] > MAXIMUM_SEQUENCE_LENGTH:\n",
    "            # If the sentence is longer than the maximum no. of allowed tokens, skip it.\n",
    "            break\n",
    "        \n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        attention_matrix = outputs.attentions\n",
    "        \n",
    "        encoding = tokenizer.encode(sentence)\n",
    "        labels = tokenizer.convert_ids_to_tokens(encoding)\n",
    "\n",
    "        i = 0\n",
    "        averages_idx = []\n",
    "        while i < len(labels) - 1:\n",
    "            j = i + 1\n",
    "            average_idx = []\n",
    "            while labels[j].startswith('#'):\n",
    "                average_idx.append(j)\n",
    "                labels[i] += labels[j].replace('#', '')\n",
    "                j += 1\n",
    "            if average_idx != []:\n",
    "                average_idx.append(i)\n",
    "                averages_idx.append(average_idx)\n",
    "            i = j\n",
    "        \n",
    "        # Construct multiple masks to indentify uninformative tokens:\n",
    "        ## i) subwords: Start with '##;'\n",
    "        ## ii) punctuation: Use string.punctuation to identify them;\n",
    "        ## iii) other: Uninformative characters that are not part of 'string.punctuation;'\n",
    "        ## iv) stop words: Use 'stopwords' from 'nltk.corpus.'\n",
    "        # Then apply OR to construct global mask of uninformative tokens.\n",
    "        hashtag_idx = np.array([label.startswith(\"#\") for label in labels])\n",
    "        stopwords_idx = np.array([label in stopwords.words('english') for label in labels])\n",
    "        punctuation_idx = np.array([label in string.punctuation for label in labels])\n",
    "        dash_idx = np.array([(len(label) == 1 and ord(label) == 8211) for label in labels])\n",
    "        # Parameter stop words: Remove or leave in.\n",
    "                \n",
    "        # Remove uninformative tokens from sentence\n",
    "        # by applying global mask.\n",
    "        remove_idx = hashtag_idx | punctuation_idx | dash_idx | stopwords_idx  \n",
    "        labels = np.array(labels)[~remove_idx]\n",
    "        # Remove '[CLS]' and '[SEP]' tokens from sentence tokens.\n",
    "        labels = labels[1:(len(labels) - 1)]\n",
    "\n",
    "        layer = 0 # Parameter: Encoder layer\n",
    "        for layer in range(12): # Parameter: Encoder layers\n",
    "            for head in range(12):\n",
    "                head_scores_raw_tensor = attention_matrix[layer][0][head].clone()\n",
    "                \n",
    "                # Remove self-attention matrix entries (rows & columns) of uninformative tokens.\n",
    "                head_scores_raw_tensor = preprocess_attention_scores(head_scores_raw_tensor, averages_idx, remove_idx)\n",
    "\n",
    "                head_scores_raw = head_scores_raw_tensor.numpy()\n",
    "                \n",
    "                # Remove entries (rows & columns) associated with '[CLS]' and '[SEP]' tokens.\n",
    "                head_scores = head_scores_raw[1:(len(head_scores_raw) - 1), 1:(len(head_scores_raw) - 1)].copy()\n",
    "\n",
    "                # Zero out entries with an attention weight\n",
    "                # lower than the attention score threshold.\n",
    "                head_scores[head_scores < as_threshold] = 0\n",
    "                \n",
    "                # Construct graph from matrix.\n",
    "                G = nx.from_numpy_array(head_scores, create_using=nx.DiGraph())\n",
    "                \n",
    "                # Construct subsequences and calculate associated\n",
    "                # chunk scores (i.e., averages of the associated attention weights).\n",
    "                # ----\n",
    "                # sequences: binary vector where the \n",
    "                # 1-components indicate the tokens that are part of the subsequence;\n",
    "                # means: float vector with the chunk scores.\n",
    "                sequences = []\n",
    "                means = []\n",
    "                n_tokens = len(labels)\n",
    "                construct_sequences(G, n_tokens)\n",
    "                \n",
    "                # Construct dataframe from subsequences.\n",
    "                df = pd.DataFrame(data=[sequences, means]).T.rename(columns={0: 'seq',  1: 'score'})\n",
    "                    \n",
    "                if len(df) > 0:\n",
    "                    # Get subsequence length.\n",
    "                    df['len'] = df['seq'].map(sum)\n",
    "                    df['score'] = df['score'].astype('float64')\n",
    "                    # Parameter: subsequence sorting: Length and then Chunk score\n",
    "                    df = df.sort_values(by=['len', 'score'], ascending=[False, False]).reset_index(drop=True)\n",
    "                    # Parameter:  Choose how many subsequences to save to memory.\n",
    "                    top3_df = df.head(1) \n",
    "                    \n",
    "                    # Save sequences w/ chunk scores to memory.\n",
    "                    for i in range(len(top3_df)):\n",
    "                        # Call 'generate_query' to construct token superposition.\n",
    "                        memory.save(\n",
    "                            inference.generate_query(\n",
    "                                address_size,\n",
    "                                cleanup,\n",
    "                                labels[top3_df['seq'][i].astype(bool)]\n",
    "                            ),\n",
    "                            top3_df['score'][i]\n",
    "                        )\n",
    "        # If prune_mode is set, prune memory.\n",
    "        memory.prune()\n",
    "#     if (pos + 1) % 50 == 0:\n",
    "#         remove_duplicates(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_memory(cleanup, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d88b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in = [\n",
    "    \"\"\"Blaine was reared in a Prohibition home, and while still a young girl, she became a very active participant at temperance meetings, where she won great favor for her songs and recitations.\"\"\",\n",
    "    \"\"\"In 1910, she was elected to the position of organizer and lecturer of the National WCTU.\"\"\",\n",
    "    \"\"\"Another feature of her work was the organization of temperance mass-meetings of Sunday-school children, usually preceded by a formal parade.\"\"\",\n",
    "    \"\"\"With all other games played, a victory over Everton had put United top of the group on nine points.\"\"\",\n",
    "    \"\"\"The 2022 FA Women's League Cup Final was the 11th final of the FA Women's League Cup, England's secondary cup competition for women's football teams and its primary league cup tournament.\"\"\",\n",
    "    \"\"\"In 2020 Mico's single 'igare' awarded as the best song of the summer in Kiss Summer Awards.\"\"\",\n",
    "    \"\"\"She collected the speech and words of Dublin city and donated her collection to the Department of Irish Folklore at University College, Dublin.\"\"\",\n",
    "    \"\"\"Traditional palyanytsya was baked from yeast dough.\"\"\",\n",
    "    \"\"\"First, hops were boiled in a pot, which was then poured into a makitra, to which sifted wheat flour was added.\"\"\",\n",
    "    \"\"\" Jonathan Holland of ScreenDaily deemed the film to be \"superbly directed by Palomero, who seems to have a special gift for seeing the world through children's eyes.\" \"\"\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c556b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### Concept extraction\n",
    "<ins>**Parameters:**</ins>\n",
    "1. `retrieve_mode`, with values `top_k` and `pooling`;\n",
    "\n",
    "The value\n",
    "- `top_k` corresponds to the return of the most similar (in the sense of the cosine similarity) `k` addresses found in the memory when querying it with the superposition of the inference sentence. `k` can be freely choosen, but it is currently set to 7. For each addresss, a dataframe containing the highest similarities between the address and the atomic tokens is returned. The number of atomic vectors returned can be set in the function [inference.get_similarities_to_atomic_set](https://github.com/dfichiu/ba-thesis/blob/master/src/lib/utils/inference.py)\n",
    "\n",
    "- `pooling` corresponds to the result (i.e., dataframe w/ the tokens with the highest cosine similarity) of the retrieve operation when querying the memory with the superposition of the inference sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c3ce1-9199-44a9-ae7d-e116fea894d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blaine was reared in a Prohibition home, and while still a young girl, she became a very active participant at temperance meetings, where she won great favor for her songs and recitations.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8734d3be3b34a18a8cd6b546160d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'In 1910, she was elected to the position of organizer and lecturer of the National WCTU.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8396737c4ba4f7281102b5eb9bad50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Another feature of her work was the organization of temperance mass-meetings of Sunday-school children, usually preceded by a formal parade.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fee7eb7989472885125bfc89d3a1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'With all other games played, a victory over Everton had put United top of the group on nine points.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inference ###\n",
    "retrieve_mode = \"top_k\" \n",
    "\n",
    "# Get table with token similarities.\n",
    "retrieved_contents = inference.infer(\n",
    "    memory.address_size,\n",
    "    cleanup,\n",
    "    memory,\n",
    "    inference_sentences_in,\n",
    "    retrieve_mode=retrieve_mode,\n",
    "    k=7,\n",
    ")\n",
    "\n",
    "if retrieve_mode == \"top_k\":\n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "    \n",
    "    for s, addresses in zip(inference_sentences_in, retrieved_contents):\n",
    "        display(s)\n",
    "        out_tables = []\n",
    "        for a in addresses:\n",
    "            address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "                a, cleanup)\n",
    "            out = widgets.Output()\n",
    "            with out:\n",
    "                display(address_sims_df)\n",
    "            out_tables.append(out)\n",
    "        display(widgets.HBox(out_tables))\n",
    "elif retrieve_mode == \"pooling\":  \n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "      \n",
    "    for s, c in zip(inference_sentences_in, retrieved_contents):\n",
    "        sentence_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            c, cleanup)\n",
    "        sentence_sims_df['sentence'] = [s] * len(sentence_sims_df)\n",
    "        sims_df = pd.concat([sims_df, sentence_sims_df])\n",
    "\n",
    "    sims_df = sims_df.sort_values(['sentence', 'similarity'], ascending=False) \\\n",
    "                     .set_index(['sentence', 'token'])\n",
    "    \n",
    "    display(sims_df)\n",
    "else:  # unrecognized\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5ee5f",
   "metadata": {},
   "source": [
    "### Memory visualization\n",
    "Visualize 30 randomly selected memory addresses. Visualize refers to recovering the atomic tokens (w/ their cosine similarity) from the superposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of existing memory addresses: {len(memory.addresses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of memory expansions: {memory.n_expansions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of memory updates: {memory.n_updates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5f82d-3a7b-4821-a545-1ef1df9e359e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "addresses = np.random.randint(0, len(memory.addresses), size=30)\n",
    "\n",
    "for address in addresses:\n",
    "    display(md(f\"### <ins>Address {address}</ins>\"))\n",
    "    display(md(f\"Address **chunk score:** {memory.scores[address][0]}, **bin score:** {memory.scores[address][1]}\"))\n",
    "    address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            memory.addresses[address],\n",
    "            cleanup,\n",
    "    )\n",
    "    display(address_sims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8d3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load pre-trained word embeddings (Word2Vec in this example)\n",
    "# word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# address_embeddings = []\n",
    "# address_concepts = []\n",
    "# addresses = []\n",
    "# bins = []\n",
    "# chunk_scores = []\n",
    "\n",
    "# for idx, address in enumerate(memory.addresses):\n",
    "#     tokens = inference.get_most_similar_HVs(inference.get_similarities_to_atomic_set(address, cleanup))\n",
    "#     embeddings = [word_vectors[word] for word in tokens if word in word_vectors]\n",
    "#     if embeddings:\n",
    "#         addresses.append(idx)\n",
    "#         bins.append(memory.scores[idx, 1].item())\n",
    "#         chunk_scores.append(memory.scores[idx, 0].item())\n",
    "#         address_concepts.append(\" \".join(tokens))\n",
    "#         address_embeddings.append(sum(embeddings) / len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_embeddings = TSNE(n_components=2, random_state=42, perplexity=2).fit_transform(np.array(address_embeddings))\n",
    "\n",
    "# df = pd.DataFrame(reduced_embeddings, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
    "# df[\"Address\"] = addresses\n",
    "# df[\"Chunk\"] = address_concepts\n",
    "# df['Bin'] = bins\n",
    "# df['Chunk-score'] = chunk_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e582d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# fig = px.scatter(\n",
    "#     df, x=\"Dimension 1\", y=\"Dimension 2\",\n",
    "#     text=\"Chunk\", hover_data=[\"Address\", \"Bin\", \"Chunk-score\"],\n",
    "#     title=\"Memory concepts\"\n",
    "# )\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
