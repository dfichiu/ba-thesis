{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e1a382-932e-4312-b5d6-f7b2bbe4518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ffe562-2195-4dbb-af53-60350a4b3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown as md\n",
    "import itertools\n",
    "\n",
    "from lib.memory import DSDM\n",
    "from lib.utils import configs, inference, learning, preprocess, utils \n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69f820f-3847-458d-a11a-875f02f02552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"  # Has 12 layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cc3565-e6aa-4498-a32e-d375ea6a473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (941 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text = \"'Albert Camus ( ,  ; ; 7 November 1913 – 4 January 1960) was a French philosopher, author, and journalist. He was awarded the 1957 Nobel Prize in Literature at the age of 44, the second-youngest recipient in history. His works include The Stranger, The Plague, The Myth of Sisyphus, The Fall, and The Rebel.\\n\\nCamus was born in French Algeria to Pieds Noirs parents. He spent his childhood in a poor neighbourhood and later studied philosophy at the University of Algiers. He was in Paris when the Germans invaded France during World War II in 1940. Camus tried to flee but finally joined the French Resistance where he served as editor-in-chief at Combat, an outlawed newspaper. After the war, he was a celebrity figure and gave many lectures around the world. He married twice but had many extramarital affairs. Camus was politically active; he was part of the left that opposed the Soviet Union because of its totalitarianism. Camus was a moralist and leaned towards anarcho-syndicalism. He was part of many organisations seeking European integration. During the Algerian War (1954–1962), he kept a neutral stance, advocating for a multicultural and pluralistic Algeria, a position that caused controversy and was rejected by most parties.\\n\\nPhilosophically, Camus\\'s views contributed to the rise of the philosophy known as absurdism. He is also considered to be an existentialist, even though he firmly rejected the term throughout his lifetime.\\n\\nLife\\n\\nEarly years and education\\n\\nAlbert Camus was born on 7 November 1913 in a working-class neighbourhood in Mondovi (present-day Dréan), in French Algeria. His mother, Catherine Hélène Camus (née Sintès), was French with Balearic Spanish ancestry. His father, Lucien Camus, a poor French agricultural worker, died in the Battle of the Marne in 1914 during World War I. Camus never knew him. Camus, his mother and other relatives lived without many basic material possessions during his childhood in the Belcourt section of Algiers. He was a second-generation French in Algeria, a French territory from 1830 until 1962. His paternal grandfather, along with many others of his generation, had moved to Algeria for a better life during the first decades of the 19th century. Hence, he was called —a slang term for French who were born in Algeria—and his identity and his poor background had a substantial effect on his later life. Nevertheless, Camus was a French citizen and enjoyed more rights than Arab and Berber Algerians under indigénat. During his childhood, Camus developed a love for football and swimming.\\n\\nUnder the influence of his teacher Louis Germain, Camus gained a scholarship in 1924 to continue his studies at a prestigious lyceum (secondary school) near Algiers. In 1930, he was diagnosed with tuberculosis. Because it is a transmitted disease, he moved out of his home and stayed with his uncle Gustave Acault, a butcher, who influenced the young Camus. It was at that time that Camus turned to philosophy, with the mentoring of his philosophy teacher Jean Grenier. He was impressed by ancient Greek philosophers and Friedrich Nietzsche. During that time, he was only able to study part-time. To earn money, he took odd jobs: as a private tutor, car parts clerk, and assistant at the Meteorological Institute.\\n\\nIn 1933, Camus enrolled at the University of Algiers and completed his licence de philosophie (BA) in 1936; after presenting his thesis on Plotinus. Camus developed an interest in early Christian philosophers, but Nietzsche and Arthur Schopenhauer had paved the way towards pessimism and atheism. Camus also studied novelist-philosophers such as Stendhal, Herman Melville, Fyodor Dostoyevsky, and Franz Kafka. In 1933, he also met Simone Hié, then a partner of a friend of Camus, who would become his first wife.\\n\\nCamus played goalkeeper for the Racing Universitaire d\\'Alger junior team from 1928 to 1930. The sense of team spirit, fraternity, and common purpose appealed to Camus enormously. In match reports, he was often praised for playing with passion and courage. Any football ambitions disappeared when he contracted tuberculosis at the age of 17. Camus drew parallels among football, human existence, morality, and personal identity. For him, the simplistic morality of football contradicted the complicated morality imposed by authorities such as the state and Church.\\n\\nFormative years\\nIn 1934, aged 20, Camus was in a relationship with Simone Hié.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861f3262-c7d5-40e6-ae53-8bc4c9ef9109",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (941) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m attention_matrix \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mattentions\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ba-thesis-YAFM42rh-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ba-thesis-YAFM42rh-py3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ba-thesis-YAFM42rh-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ba-thesis-YAFM42rh-py3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 238\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    240\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (941) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs, output_attentions=True)\n",
    "attention_matrix = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f68dd8-585e-4c74-86af-1285a4b9949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(text)\n",
    "labels = tokenizer.convert_ids_to_tokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1676ba-e033-4722-a455-3249994dc337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5325f-1200-4796-9fe5-065bf5233077",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels, columns=['token']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afa520-02f8-4a5f-bf45-da39bbd95f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_matrix[0].shape # Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0af19-36c2-4011-834c-5b5f6492d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_matrix[0][0].shape # Attention heads, Token-Token matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cb2a6-6600-4552-9e62-0024e55a15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = 0\n",
    "\n",
    "# token_scores = attention_matrix[0][0][0][token].cpu().detach().numpy() # First token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1234a7-1777-4836-9c36-d85e52b1efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix[11][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e676b4-552d-4f14-aa95-b4587173255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix[11].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57eaae-43fa-4afb-8829-ff922e165979",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attention_matrix[0][0][head].cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4419c-81ff-4dde-8089-a15739babf73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for layer in range(12):\n",
    "layer = 0\n",
    "for head in range(12):\n",
    "    head_scores = attention_matrix[layer][0][head].cpu().detach().numpy() # First head\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(\n",
    "        head_scores,\n",
    "        linewidth=0.5,\n",
    "        #xticklabels=labels,\n",
    "        #yticklabels=labels,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "    )\n",
    "    plt.title(f'Self-attention matrix: layer {layer}, head {head}', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc36bc-54b8-4374-b190-a6f9fd8957de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ac982-9725-4df7-906e-7f47eaebfbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4e999-e9b7-452e-8331-b5738c0c36b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d453608-da8d-4a33-a38e-79062005c42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
