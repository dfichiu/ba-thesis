{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e1a382-932e-4312-b5d6-f7b2bbe4518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ffe562-2195-4dbb-af53-60350a4b3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML, Markdown as md\n",
    "import itertools\n",
    "\n",
    "from lib.memory import DSDM\n",
    "from lib.utils import cleanup, configs, inference, learning, preprocess, utils \n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Package options ###\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d388cf6-77de-40bd-83b6-c5db77acbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils ###\n",
    "def plot_heatmap(x: np.array, labels: np.array) -> None:\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(\n",
    "        x,\n",
    "        linewidth=0.5,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "    )\n",
    "    plt.title(f'Self-attention matrix: layer {layer}, head {head}', fontsize=15)\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def average_out_and_remove_rows(t: torch.tensor, averages_idx, remove_idx):\n",
    "    for average_idx in averages_idx:  # The nested lists can have different dimensions.\n",
    "        # Replace the attention scores of the first token with the average of the token attention scores.\n",
    "        t[min(average_idx)] = torch.mean(t[average_idx], dim=0, keepdim=True)\n",
    "    return t[~remove_idx]\n",
    "\n",
    "\n",
    "def preprocess_attention_scores(attention_scores, averages_idx, remove_idx):\n",
    "    attention_scores = average_out_and_remove_rows(attention_scores, averages_idx, remove_idx)\n",
    "    attention_scores = attention_scores.transpose(0, 1)\n",
    "    attention_scores = average_out_and_remove_rows(attention_scores, averages_idx, remove_idx)\n",
    "    return attention_scores.transpose(0, 1)\n",
    "        \n",
    "    \n",
    "\n",
    "def backward_pass(G, current_node, left_edge, right_edge, sequence, mean):\n",
    "    in_nodes = np.array([edge[0] for edge in list(G.in_edges(current_node))])\n",
    "    in_nodes = in_nodes[(in_nodes > left_edge) & (in_nodes < current_node)]\n",
    "    for node in in_nodes:\n",
    "        sequence[node] = 1\n",
    "        sequences.append(sequence)\n",
    "        mean += G[node][current_node]['weight']\n",
    "        means.append(round(mean / (sum(sequence) - 1), 2))\n",
    "        backward_pass(G, node, left_edge, node, sequence.copy(), mean)\n",
    "        forward_pass(G, node, left_edge, current_node, sequence.copy(), mean)\n",
    "        \n",
    "    return\n",
    "    \n",
    "    \n",
    "def forward_pass(G, current_node, left_edge, right_edge, sequence, mean):\n",
    "    out_nodes = np.array([edge[1] for edge in list(G.out_edges(current_node))])\n",
    "    out_nodes = out_nodes[(out_nodes > current_node) & (out_nodes < right_edge)]\n",
    "    for node in out_nodes:\n",
    "        sequence[node] = 1\n",
    "        mean += G[current_node][node]['weight']\n",
    "        sequences.append(sequence)\n",
    "        means.append(round(mean / (sum(sequence) - 1), 2))\n",
    "        backward_pass(G, node, current_node, node, sequence.copy(), mean)\n",
    "        forward_pass(G, node, node, right_edge, sequence.copy(), mean)\n",
    "            \n",
    "    return\n",
    "    \n",
    "\n",
    "def construct_sequences(G: nx.DiGraph, n_tokens):\n",
    "    for node in G.nodes():\n",
    "        sequence = np.zeros(n_tokens)\n",
    "        mean = 0\n",
    "        sequence[node] = 1\n",
    "        #sequences.append(sequence) # Do not allow for 1-token sequences.\n",
    "        forward_pass(G, node, node, n_tokens, sequence.copy(), mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f8bd36-2a2e-472c-b5af-ffe9ed6cc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/nfs/data/projects/daniela/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e821f214cba4c91b79c6672d60db988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Wikipedia dataset.\n",
    "# TODO: Split between server and local.\n",
    "#wiki_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.en\")['train']\n",
    "wiki_dataset = datasets.load_dataset(\n",
    "    \"wikipedia\",\n",
    "    \"20220301.en\",\n",
    "    cache_dir=\"/nfs/data/projects/daniela\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6025b42d-acd1-4bc3-9a13-9069a0747f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Using seed: 41"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "utils.fix_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d79c11-11fa-4160-ae31-0b5d9978f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DSDM hyperparameters.\n",
    "address_size = 1000\n",
    "ema_time_period = 7000 # 500\n",
    "learning_rate_update = 0.5\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "normalize = False\n",
    "\n",
    "#chunk_sizes = [5]\n",
    "\n",
    "prune_mode = \"fixed-size\"\n",
    "max_size_address_space = 3000\n",
    "chunk_score_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f277d1-3913-4c25-8b78-e6526c0ea5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = cleanup.Cleanup(address_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69f820f-3847-458d-a11a-875f02f02552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"  # Has 12 layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "MAXIMUM_SEQUENCE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d2626b-fe82-4bfe-9c9f-8492650e4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory.\n",
    "memory = DSDM.DSDM(\n",
    "    address_size=address_size,\n",
    "    ema_time_period=ema_time_period,\n",
    "    learning_rate_update=learning_rate_update,\n",
    "    temperature=temperature,\n",
    "    normalize=normalize,\n",
    "    prune_mode=prune_mode,\n",
    "    max_size_address_space=max_size_address_space,\n",
    "    chunk_score_threshold=chunk_score_threshold,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794da007-b59f-4b49-ab0b-ef55f3b1da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct train set (texts) and inference set (sentences; in and out of train set text).\n",
    "train_size = 10\n",
    "test_size = 10\n",
    "\n",
    "# Text indeces.\n",
    "train_idx = np.random.randint(0, len(wiki_dataset), size=train_size)\n",
    "#train_idx = np.append(np.append(np.append(train_idx[0], train_idx[0]), train_idx[0]), train_idx[0]) \n",
    "\n",
    "# Caclulate chosen text statistics.\n",
    "# TODO\n",
    "\n",
    "# Text indeces from which we extract sentences.\n",
    "intest_idx = np.random.choice(train_idx, test_size)\n",
    "outtest_idx = np.random.choice(np.setdiff1d(np.arange(len(wiki_dataset)), train_idx), test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3258e29d-1d02-4721-a547-5795a41d74b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in = []\n",
    "inference_sentences_out = []\n",
    "\n",
    "for idx_in, idx_out in zip(intest_idx, outtest_idx):\n",
    "    # Get sentences.\n",
    "    sentences_in = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_in)]['text'])\n",
    "    sentences_out = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_out)]['text'])\n",
    "    \n",
    "    # Get sentence index.\n",
    "    sentence_idx_in = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_in),\n",
    "            size=1\n",
    "        )\n",
    "    )\n",
    "    sentence_idx_out = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_out),\n",
    "            size=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Append sentence to list.\n",
    "    inference_sentences_in.append(sentences_in[sentence_idx_in])\n",
    "    inference_sentences_out.append(sentences_out[sentence_idx_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebe1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups_found = 0\n",
    "\n",
    "def remove_duplicates(memory):\n",
    "    global dups_found\n",
    "    global_keep_mask = torch.tensor([True] * len(memory.addresses)).to(device)\n",
    "    \n",
    "    for idx, address in enumerate(memory.addresses):\n",
    "        if global_keep_mask[idx].item():\n",
    "            cos = torch.nn.CosineSimilarity()\n",
    "            keep_mask = cos(memory.addresses, address) < 0.95\n",
    "            # Keep current address\n",
    "            keep_mask[idx] = True\n",
    "            global_keep_mask &= keep_mask\n",
    "\n",
    "    if global_keep_mask.sum().item() > 0:\n",
    "        dups_found += 1\n",
    "        # Remove similar addresses\n",
    "        memory.addresses = memory.addresses[global_keep_mask]\n",
    "        # Remove bins\n",
    "        memory.bins = memory.bins[global_keep_mask]\n",
    "        # Remove chunk scores\n",
    "        memory.chunk_scores = memory.chunk_scores[global_keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f395fae-f8ce-490b-80ac-a1675280b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▊                                                                                     | 4/10 [01:15<01:46, 17.79s/it]"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for pos, i in enumerate(tqdm(train_idx)):\n",
    "    text = wiki_dataset[int(i)]['text']\n",
    "    \n",
    "    # Preprocess data. \n",
    "    sentences = preprocess.split_text_into_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        if inputs['input_ids'].shape[1] > MAXIMUM_SEQUENCE_LENGTH:\n",
    "            break\n",
    "        \n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        attention_matrix = outputs.attentions\n",
    "        \n",
    "        encoding = tokenizer.encode(sentence)\n",
    "        labels = tokenizer.convert_ids_to_tokens(encoding)\n",
    "\n",
    "        i = 0\n",
    "        averages_idx = []\n",
    "        while i < len(labels) - 1:\n",
    "            j = i + 1\n",
    "            average_idx = []\n",
    "            while labels[j].startswith('#'):\n",
    "                average_idx.append(j)\n",
    "                labels[i] += labels[j].replace('#', '')\n",
    "                j += 1\n",
    "            if average_idx != []:\n",
    "                average_idx.append(i)\n",
    "                averages_idx.append(average_idx)\n",
    "            i = j\n",
    "        \n",
    "        hashtag_idx = np.array([label.startswith(\"#\") for label in labels])\n",
    "        stopwords_idx = np.array([label in stopwords.words('english') for label in labels])\n",
    "        punctuation_idx = np.array([label in string.punctuation for label in labels])\n",
    "        remove_idx = hashtag_idx | punctuation_idx | stopwords_idx\n",
    "        labels = np.array(labels)[~remove_idx]\n",
    "        labels = labels[1:(len(labels) - 1)]\n",
    "\n",
    "        layer = 0\n",
    "        for head in range(12):\n",
    "            head_scores_raw_tensor = attention_matrix[layer][0][head].detach().clone()\n",
    "        \n",
    "            head_scores_raw_tensor = preprocess_attention_scores(head_scores_raw_tensor, averages_idx, remove_idx)\n",
    "            \n",
    "            head_scores_raw = head_scores_raw_tensor.cpu().detach().numpy()\n",
    "            \n",
    "            head_scores = head_scores_raw[1:(len(head_scores_raw) - 1), 1:(len(head_scores_raw) - 1)].copy()\n",
    "        \n",
    "            as_threshold = 0.5\n",
    "            head_scores[head_scores < as_threshold] = 0\n",
    "            \n",
    "            G = nx.from_numpy_array(head_scores, create_using = nx.DiGraph())\n",
    "        \n",
    "            sequences = []\n",
    "            means = []\n",
    "            n_tokens = len(labels)\n",
    "            construct_sequences(G, n_tokens)\n",
    "                \n",
    "            df = pd.DataFrame(data=[sequences, means]).T.rename(columns={0: 'seq',  1: 'score'})\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                df['len'] = df['seq'].map(sum)\n",
    "                df['score'] = df['score'].astype('float64')\n",
    "                df = df.sort_values(by=['score', 'len'], ascending=[False, False]).reset_index(drop=True)\n",
    "                top3_df = df.head(3)\n",
    "            \n",
    "                for i in range(len(top3_df)):\n",
    "                    memory.save(\n",
    "                        inference.generate_query(\n",
    "                            address_size,\n",
    "                            cleanup,\n",
    "                            labels[top3_df['seq'][i].astype(bool)]\n",
    "                        ),\n",
    "                        top3_df['score'][i]\n",
    "                    )\n",
    "        memory.prune()\n",
    "#     if (pos + 1) % 50 == 0:\n",
    "#         remove_duplicates(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f379e1e-3f98-469c-8c28-cb33d971d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_sentences_in = [\"The Society convenes an annual conference, in locations across the United States and in Canada, usually in June, to convey the James Alice award.\"]\n",
    "inference_sentences_in = [\"Deputy director flys to the United States.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory.addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c3ce1-9199-44a9-ae7d-e116fea894d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_mode = \"top_k\"\n",
    "\n",
    "# Get table with token similarities for each \"out-of-train\" sentence.\n",
    "retrieved_contents = inference.infer(\n",
    "    memory.address_size,\n",
    "    cleanup,\n",
    "    memory,\n",
    "    inference_sentences_in,\n",
    "    retrieve_mode=retrieve_mode,\n",
    "    k=7, #TODO: What if index is out of range?\n",
    ")\n",
    "\n",
    "if retrieve_mode == \"top_k\":\n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "    \n",
    "    for s, addresses in zip(inference_sentences_in, retrieved_contents):\n",
    "        display(s)\n",
    "        out_tables = []\n",
    "        for a in addresses:\n",
    "            address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "                a, cleanup)\n",
    "            out = widgets.Output()\n",
    "            with out:\n",
    "                display(address_sims_df)\n",
    "            out_tables.append(out)\n",
    "        display(widgets.HBox(out_tables))\n",
    "elif retrieve_mode == \"pooling\":  \n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "      \n",
    "    for s, c in zip(inference_sentences_in, retrieved_contents):\n",
    "        sentence_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            c, cleanup)\n",
    "        sentence_sims_df['sentence'] = [s] * len(sentence_sims_df)\n",
    "        sims_df = pd.concat([sims_df, sentence_sims_df])\n",
    "\n",
    "    sims_df = sims_df.sort_values(['sentence', 'similarity'], ascending=False) \\\n",
    "                     .set_index(['sentence', 'token'])\n",
    "    \n",
    "    display(sims_df)\n",
    "else:  # unrecognized\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc628da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory.addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5f82d-3a7b-4821-a545-1ef1df9e359e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "memory.prune()\n",
    "addresses = np.random.randint(0, len(memory.addresses), size=70)\n",
    "#addresses = np.argwhere((memory.chunk_scores > 0.97).cpu().detach().numpy().flatten()).flatten()\n",
    "#addresses = np.argwhere((memory.bins > 50).cpu().detach().numpy().flatten()).flatten()\n",
    "\n",
    "for address in np.arange(0, len(memory.addresses)):\n",
    "    display(md(f\"### Address {address}\"))\n",
    "    address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            memory.addresses[address],\n",
    "            cleanup,\n",
    "    )\n",
    "    display(address_sims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362461c4-c57c-4832-831d-69cb118e0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.bins[np.argwhere((memory.chunk_scores > 0.97).cpu().detach().numpy().flatten()).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6eec5-d0da-447e-a540-72713252ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere((memory.bins > 120 ).cpu().detach().numpy().flatten()).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f8395-bde7-4244-a648-2739769992f8",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc3565-e6aa-4498-a32e-d375ea6a473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"couldn't.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f3262-c7d5-40e6-ae53-8bc4c9ef9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, output_attentions=True)\n",
    "attention_matrix = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f68dd8-585e-4c74-86af-1285a4b9949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(text)\n",
    "labels = tokenizer.convert_ids_to_tokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c3fc1-a895-478e-a66f-a35df9c609ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "averages_idx = []\n",
    "while i < len(labels) - 1:\n",
    "    j = i + 1\n",
    "    average_idx = []\n",
    "    while labels[j].startswith('#'):\n",
    "        average_idx.append(j)\n",
    "        labels[i] += labels[j].replace('#', '')\n",
    "        j += 1\n",
    "    if average_idx != []:\n",
    "        average_idx.append(i)\n",
    "        averages_idx.append(average_idx)\n",
    "    i = j\n",
    "\n",
    "hashtag_idx = np.array([label.startswith(\"#\") for label in labels])\n",
    "punctuation_idx = np.array([label in string.punctuation for label in labels])\n",
    "remove_idx = hashtag_idx | punctuation_idx\n",
    "labels = np.array(labels)[~remove_idx]\n",
    "labels = labels[1:(len(labels) - 1)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4419c-81ff-4dde-8089-a15739babf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "\n",
    "for head in range(12):\n",
    "    head_scores_raw_tensor = attention_matrix[layer][0][head].detach().clone()\n",
    "    \n",
    "    head_scores_raw_tensor = preprocess_attention_scores(head_scores_raw_tensor, averages_idx, remove_idx)s\n",
    "        \n",
    "    head_scores_raw = head_scores_raw_tensor.cpu().detach().numpy()\n",
    "    \n",
    "    head_scores = head_scores_raw[1:(len(head_scores_raw) - 1), 1:(len(head_scores_raw) - 1)].copy()s\n",
    "\n",
    "    as_threshold = 0.4\n",
    "    head_scores[head_scores < as_threshold] = 0\n",
    "    plot_heatmap(head_scores, labels)\n",
    "    \n",
    "    G = nx.from_numpy_array(head_scores, create_using = nx.DiGraph())\n",
    "    G.edges.data()\n",
    "\n",
    "    sequences = []\n",
    "    #mean_scores = []\n",
    "    n_tokens = len(labels)\n",
    "    construct_sequences(G, n_tokens)\n",
    "    # for seq in sequences:\n",
    "    #     idx = list(itertools.chain(*np.argwhere(seq == 1)))\n",
    "    #     mean = 0\n",
    "    #     for i, j in zip(idx[:-1],  idx[1:]):\n",
    "    #         mean += G[i][j]['weight']\n",
    "    #     mean /= (len(idx) - 1)\n",
    "    #     mean_scores.append(round(mean, 2))\n",
    "        \n",
    "    # df = pd.DataFrame(data=[sequences, mean_scores]).T.rename(columns={0: 'seq',  1: 'score'})\n",
    "    # if len(df) > 0:\n",
    "    #     df['len'] = df['seq'].map(sum)\n",
    "    #     df['score'] = df['score'].astype('float64')\n",
    "    #     df = df.sort_values(by=['score', 'len'], ascending=[False, False]).reset_index(drop=True)\n",
    "    #     top3_df = df.head(3)\n",
    "    #     display(df)\n",
    "    \n",
    "    #     for i in range(len(top3_df)):\n",
    "    #         print(labels[top3_df['seq'][i].astype(bool)], top3_df['score'][i])\n",
    "    \n",
    "    #if sequences != []:\n",
    "        #layer_sequences.append(sequences)\n",
    "    if sequences != []:\n",
    "        print(head)\n",
    "        for seq in sequences:\n",
    "            print(labels[seq.astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac15db-2276-4567-9301-e21dbe95887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Firenze firenze\"\n",
    "# encoding = tokenizer.encode(text)\n",
    "# labels = tokenizer.convert_ids_to_tokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca9d95-e3d3-4590-a994-3b59e06aad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# averages_idx = []\n",
    "# while i < len(labels) - 1:\n",
    "#     j = i + 1\n",
    "#     average_idx = []\n",
    "#     while labels[j].startswith('#'):\n",
    "#         average_idx.append(j)\n",
    "#         labels[i] += labels[j].replace('#', '')\n",
    "#         j += 1\n",
    "#     if average_idx != []:\n",
    "#         average_idx.append(i)\n",
    "#         averages_idx.append(average_idx)\n",
    "#     i = j\n",
    "\n",
    "# hashtag_idx = np.array([label.startswith(\"#\") for label in labels])\n",
    "# labels = np.array(labels)[~hashtag_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e7855-7fb3-4a04-91d0-8e42cabd1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch implementation.\n",
    "\n",
    "# t = torch.tensor(head_scores_raw)\n",
    "# i = torch.tensor(averages_idx)\n",
    "\n",
    "# t[i] = torch.mean(t[i], dim=1, keepdim=True)\n",
    "# t = torch.unique_consecutive(t, dim=0)\n",
    "# t = torch.transpose(t, 0, 1)\n",
    "# t[i] = torch.mean(t[i], dim=1, keepdim=True)\n",
    "# t = torch.unique_consecutive(t, dim=0)\n",
    "\n",
    "# t = torch.transpose(t, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a05c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
