{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [],
   "source": [
    "import datasets #import load_dataset\n",
    "\n",
    "from IPython.display import HTML, Markdown as md\n",
    "import itertools\n",
    "\n",
    "from lib.memory import DSDM\n",
    "from lib.utils import configs, inference, learning, preprocess, utils \n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm\n",
    "# Type checking\n",
    "from typing import List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/Users/danielastelea/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5f6d09684d46fe959e06bbda92dcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Wikipedia dataset.\n",
    "wiki_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.en\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Using seed: 41"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "utils.fix_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = {} # Cleanup memory for saving atomic HVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6iuLthXgNKLP",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set DSDM hyperparameters.\n",
    "address_size = 1000\n",
    "ema_time_period = 5000\n",
    "learning_rate_update = 0.5\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "normalize = False\n",
    "\n",
    "chunk_sizes = [5]\n",
    "\n",
    "prune_mode = \"fixed-size\"\n",
    "max_size_address_space = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory.\n",
    "memory = DSDM.DSDM(\n",
    "    address_size=address_size,\n",
    "    ema_time_period=ema_time_period,\n",
    "    learning_rate_update=learning_rate_update,\n",
    "    temperature=temperature,\n",
    "    normalize=normalize,\n",
    "    prune_mode=prune_mode,\n",
    "    max_size_address_space=max_size_address_space\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct train set (texts) and inference set (sentences; in and out of train set text).\n",
    "train_size = 3\n",
    "test_size = 1\n",
    "\n",
    "# Text indeces.\n",
    "train_idx = np.random.randint(0, len(wiki_dataset), size=train_size)\n",
    "\n",
    "# Caclulate chosen text statistics.\n",
    "# TODO\n",
    "\n",
    "# Text indeces from which we extract sentences.\n",
    "intest_idx = np.random.choice(train_idx, test_size)\n",
    "outtest_idx = np.random.choice(np.setdiff1d(np.arange(len(wiki_dataset)), train_idx), test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in = []\n",
    "inference_sentences_out = []\n",
    "\n",
    "for idx_in, idx_out in zip(intest_idx, outtest_idx):\n",
    "    # Get sentences.\n",
    "    sentences_in = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_in)]['text'])\n",
    "    sentences_out = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_out)]['text'])\n",
    "    \n",
    "    # Get sentence index.\n",
    "    sentence_idx_in = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_in),\n",
    "            size=1\n",
    "        ))\n",
    "    sentence_idx_out = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_out),\n",
    "            size=1\n",
    "        ))\n",
    "\n",
    "    # Append sentence to list.\n",
    "    inference_sentences_in.append(sentences_in[sentence_idx_in])\n",
    "    inference_sentences_out.append(sentences_out[sentence_idx_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 21.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for i in tqdm(train_idx):\n",
    "    text = wiki_dataset[int(i)]['text']\n",
    "    \n",
    "    # Preprocess data. \n",
    "    sentences_tokens = preprocess.preprocess_text(text)\n",
    "    \n",
    "    for sentence_tokens in sentences_tokens:\n",
    "        # Generate atomic HVs for unknown tokens.\n",
    "        learning.generate_atomic_HVs_from_tokens_and_add_them_to_cleanup(\n",
    "            memory.address_size,\n",
    "            cleanup,\n",
    "            sentence_tokens\n",
    "        )\n",
    "        \n",
    "        # Learning: Construct the chunks of each sentence and save them to memory.\n",
    "        learning.generate_chunk_representations_and_save_them_to_memory(\n",
    "            memory.address_size,\n",
    "            cleanup,\n",
    "            memory,\n",
    "            sentence_tokens,\n",
    "            chunk_sizes=chunk_sizes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.767993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.411019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>0.401306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>varies</td>\n",
       "      <td>0.369040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sin</td>\n",
       "      <td>0.075006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.073614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>directed</td>\n",
       "      <td>0.070625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amador</td>\n",
       "      <td>0.069162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>issued</td>\n",
       "      <td>0.064870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drama</td>\n",
       "      <td>0.062544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  similarity\n",
       "0        mm    0.767993\n",
       "1        13    0.411019\n",
       "2        19    0.401306\n",
       "3    varies    0.369040\n",
       "4       sin    0.075006\n",
       "5       sea    0.073614\n",
       "6  directed    0.070625\n",
       "7    amador    0.069162\n",
       "8    issued    0.064870\n",
       "9     drama    0.062544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.769497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0.423399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diameter</td>\n",
       "      <td>0.392815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.386750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issued</td>\n",
       "      <td>0.112748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nico</td>\n",
       "      <td>0.085273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>often</td>\n",
       "      <td>0.084291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>whorls</td>\n",
       "      <td>0.068009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>directed</td>\n",
       "      <td>0.062003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>australia</td>\n",
       "      <td>0.055716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  similarity\n",
       "0         mm    0.769497\n",
       "1         19    0.423399\n",
       "2   diameter    0.392815\n",
       "3         13    0.386750\n",
       "4     issued    0.112748\n",
       "5       nico    0.085273\n",
       "6      often    0.084291\n",
       "7     whorls    0.068009\n",
       "8   directed    0.062003\n",
       "9  australia    0.055716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieve_mode = \"top_k\"\n",
    "\n",
    "# Get table with token similarities for each \"in-train\" sentence.\n",
    "retrieved_contents = inference.infer(\n",
    "    memory.address_size,\n",
    "    cleanup,\n",
    "    memory,\n",
    "    inference_sentences_in,\n",
    "    retrieve_mode=retrieve_mode,\n",
    "    k=2,\n",
    ")\n",
    "if retrieve_mode == \"top_k\":\n",
    "    for retrieved_content in retrieved_contents[0]:\n",
    "        memory_sims_df = pd.DataFrame(columns=['token', 'similarity'])\n",
    "        \n",
    "        for token, atomic_HC in cleanup.items():\n",
    "            memory_sims_df = pd.concat([memory_sims_df, pd.DataFrame([{'token': token,\n",
    "                                                                       'similarity': thd.cosine_similarity(atomic_HC, retrieved_content).item()}])])\n",
    "        memory_sims_df = memory_sims_df.sort_values('similarity', ascending=False).reset_index(drop=True)\n",
    "        display(memory_sims_df.head(10))\n",
    "else:\n",
    "    display(retrieved_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3448390241.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    for retrieved_content in retrieved_content[0]s:\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Get table with token similarities for each \"out-of-train\" sentence.\n",
    "retrieved_contents = inference.infer(\n",
    "    memory.address_size,\n",
    "    cleanup,\n",
    "    memory,\n",
    "    inference_sentences_out,\n",
    "    retrieve_mode=\"top_k\",\n",
    "    k=1, #TODO: What if index is out of range?\n",
    ")\n",
    "if retrieve_mode == \"top_k\":\n",
    "    for retrieved_content in retrieved_contents[0]:\n",
    "        for token, atomic_HC in cleanup.items():\n",
    "            memory_sims_df = pd.concat([memory_sims_df, pd.DataFrame([{'token': token,\n",
    "                                                                       'similarity': thd.cosine_similarity(atomic_HC, retrieved_content).item()}])])\n",
    "        memory_sims_df = memory_sims_df.sort_values('similarity', ascending=False).reset_index(drop=True)\n",
    "        display(memory_sims_df.head(10))\n",
    "else:\n",
    "    display(retrieved_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = np.random.randint(0, len(memory.addresses), size=10)\n",
    "\n",
    "for address in addresses:\n",
    "    display(md(f\"### Address {address}\"))\n",
    "    retrieved_content = memory.addresses[address]\n",
    "    \n",
    "    memory_sims_df = pd.DataFrame(columns=['token', 'similarity'])\n",
    "    \n",
    "    for token, atomic_HC in cleanup.items():\n",
    "        memory_sims_df = pd.concat([memory_sims_df, pd.DataFrame([{'token': token,\n",
    "                                                                   'similarity': thd.cosine_similarity(atomic_HC, retrieved_content).item()}])])\n",
    "    memory_sims_df = memory_sims_df.sort_values('similarity', ascending=False).reset_index(drop=True)\n",
    "    display(memory_sims_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_updates / (memory.n_updates + memory.n_expansions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory.addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
