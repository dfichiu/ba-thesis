{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, Markdown \u001b[38;5;28;01mas\u001b[39;00m md\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "from IPython.display import HTML, Markdown as md\n",
    "import itertools\n",
    "\n",
    "from lib.memory import DSDM\n",
    "from lib.utils import cleanup, configs, inference, learning, preprocess, utils \n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm\n",
    "# Type checking\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wikipedia dataset.\n",
    "# TODO: Split between server and local.\n",
    "#wiki_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.en\")['train']\n",
    "wiki_dataset = datasets.load_dataset(\n",
    "    \"wikipedia\",\n",
    "    \"20220301.en\",\n",
    "    cache_dir=\"/nfs/data/projects/daniela\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "utils.fix_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iuLthXgNKLP",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set DSDM hyperparameters.\n",
    "address_size = 1000\n",
    "ema_time_period = 5000\n",
    "learning_rate_update = 0.5\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "normalize = False\n",
    "\n",
    "chunk_sizes = [5]\n",
    "\n",
    "prune_mode = \"fixed-size\"\n",
    "max_size_address_space = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = cleanup.Cleanup(address_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory.\n",
    "memory = DSDM.DSDM(\n",
    "    address_size=address_size,\n",
    "    ema_time_period=ema_time_period,\n",
    "    learning_rate_update=learning_rate_update,\n",
    "    temperature=temperature,\n",
    "    normalize=normalize,\n",
    "    prune_mode=prune_mode,\n",
    "    max_size_address_space=max_size_address_space\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct train set (texts) and inference set (sentences; in and out of train set text).\n",
    "train_size = 10\n",
    "test_size = 10\n",
    "\n",
    "# Text indeces.\n",
    "train_idx = np.random.randint(0, len(wiki_dataset), size=train_size)\n",
    "\n",
    "# Caclulate chosen text statistics.\n",
    "# TODO\n",
    "\n",
    "# Text indeces from which we extract sentences.\n",
    "intest_idx = np.random.choice(train_idx, test_size)\n",
    "outtest_idx = np.random.choice(np.setdiff1d(np.arange(len(wiki_dataset)), train_idx), test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in = []\n",
    "inference_sentences_out = []\n",
    "\n",
    "for idx_in, idx_out in zip(intest_idx, outtest_idx):\n",
    "    # Get sentences.\n",
    "    sentences_in = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_in)]['text'])\n",
    "    sentences_out = utils.preprocess.split_text_into_sentences(wiki_dataset[int(idx_out)]['text'])\n",
    "    \n",
    "    # Get sentence index.\n",
    "    sentence_idx_in = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_in),\n",
    "            size=1\n",
    "        )\n",
    "    )\n",
    "    sentence_idx_out = int(\n",
    "        np.random.randint(\n",
    "            0,\n",
    "            len(sentences_out),\n",
    "            size=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Append sentence to list.\n",
    "    inference_sentences_in.append(sentences_in[sentence_idx_in])\n",
    "    inference_sentences_out.append(sentences_out[sentence_idx_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for i in tqdm(train_idx):\n",
    "    text = wiki_dataset[int(i)]['text']\n",
    "    \n",
    "    # Preprocess data. \n",
    "    sentences_tokens = preprocess.preprocess_text(text)\n",
    "    \n",
    "    for sentence_tokens in sentences_tokens:\n",
    "        # Generate atomic HVs for unknown tokens.\n",
    "        learning.generate_atomic_HVs_from_tokens_and_add_them_to_cleanup(\n",
    "            memory.address_size,\n",
    "            cleanup,\n",
    "            sentence_tokens\n",
    "        )\n",
    "        \n",
    "        # Learning: Construct the chunks of each sentence and save them to memory.\n",
    "        learning.generate_chunk_representations_and_save_them_to_memory(\n",
    "            memory.address_size,\n",
    "            cleanup,\n",
    "            memory,\n",
    "            sentence_tokens,\n",
    "            chunk_sizes=chunk_sizes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_sentences_in = ['Dagored', 'is an Italian', 'record labels', 'based in Firenze', 'formed', 'in 1998.'] 250, 0.05 temperature\n",
    "# 'record labels' also caught by transformer attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_partition(input_partition, output_partition):\n",
    "#     # Note: What if a sentence contains the same word multiple times? This is why using 'set' is  bad idea!\n",
    "#     set_query = set(preprocess.remove_stopwords(tokens)[0]) \n",
    "#     set_content = inference.get_most_similar_HVs(sentence_sims_df, delta_threshold=0.1)\n",
    "\n",
    "#     set_input = set(input_partition)\n",
    "#     set_output = set(output_partition)\n",
    "    \n",
    "#     score = len(set_input.intersection(set_output)) / len(set_input)\n",
    "\n",
    "#     return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def divide_and_conquer(token_partitions: typing.List[typing.List[str]]):\n",
    "#     retrieve_mode = \"pooling\"\n",
    "    \n",
    "#     for tp in token_partitions:\n",
    "#         retrieved_content = inference.infer(\n",
    "#             memory.address_size,\n",
    "#             cleanup,\n",
    "#             memory,\n",
    "#             [tp],\n",
    "#             retrieve_mode=retrieve_mode\n",
    "#         )\n",
    "#         output_tokens = inference.get_most_similar_HVs(\n",
    "#             inference.get_similarities_to_atomic_set(\n",
    "#                 retrieved_contents[0],\n",
    "#                 cleanup,\n",
    "#             ),\n",
    "#             delta_threshold=0.1\n",
    "#         )\n",
    "#         score = score_partition(tp, output_tokens)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#     display(score)\n",
    "#     if score == 1:\n",
    "#         return tokens\n",
    "#     else:\n",
    "#         return max(score, divide_and_conquer())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide_and_conquer(\"Record labels from all over the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in = [\"Dagored is an Italian record label based in Firenze, formed in 1998.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_mode = \"top_k\"\n",
    "\n",
    "# Get table with token similarities for each \"out-of-train\" sentence.\n",
    "retrieved_contents = inference.infer(\n",
    "    memory.address_size,\n",
    "    cleanup,\n",
    "    memory,\n",
    "    inference_sentences_in,\n",
    "    retrieve_mode=retrieve_mode,\n",
    "    k=3, #TODO: What if index is out of range?\n",
    ")\n",
    "\n",
    "if retrieve_mode == \"top_k\":\n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "    \n",
    "    for s, addresses in zip(inference_sentences_in, retrieved_contents):\n",
    "        display(s)\n",
    "        for a in addresses:\n",
    "            address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "                a, cleanup)\n",
    "            display(address_sims_df)\n",
    "elif retrieve_mode == \"pooling\":  \n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity']) \n",
    "      \n",
    "    for s, c in zip(inference_sentences_in, retrieved_contents):\n",
    "        sentence_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            c, cleanup)\n",
    "        sentence_sims_df['sentence'] = [s] * len(sentence_sims_df)\n",
    "        sims_df = pd.concat([sims_df, sentence_sims_df])\n",
    "\n",
    "    sims_df = sims_df.sort_values(['sentence', 'similarity'], ascending=False) \\\n",
    "                     .set_index(['sentence', 'token'])\n",
    "    \n",
    "    display(sims_df)\n",
    "else:  # unrecognized\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = np.random.randint(0, len(memory.addresses), size=30)\n",
    "\n",
    "for address in addresses:\n",
    "    display(md(f\"### Address {address}\"))\n",
    "    address_sims_df = inference.get_similarities_to_atomic_set(\n",
    "            memory.addresses[address],\n",
    "            cleanup,\n",
    "    )\n",
    "    display(address_sims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_updates / (memory.n_updates + memory.n_expansions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory.addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.n_deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sentences_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
