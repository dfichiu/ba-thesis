{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDwnkbc5n-8C"
   },
   "source": [
    "# DSDM \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown as md\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6iuLthXgNKLP"
   },
   "outputs": [],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Vector dimension\n",
    "dim = 2000 \n",
    "\n",
    "cleanup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "txy0zDE7Zk4K"
   },
   "outputs": [],
   "source": [
    "def fix_seed():\n",
    "    seed = 42\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def load_data(path, bs=0, shuffle=False):\n",
    "    \"\"\"Load data from file path.\"\"\"\n",
    "    text = pathlib.Path(path).read_text(encoding='utf-8')\n",
    "    return text.splitlines()\n",
    "\n",
    "\n",
    "def compute_distances_gpu(X, Y):\n",
    "    \"\"\"Compute Euclidean distance.\"\"\"\n",
    "    return torch.sqrt(-2 * torch.mm(X,Y.T) +\n",
    "                    torch.sum(torch.pow(Y, 2),dim=1) +\n",
    "                    torch.sum(torch.pow(X, 2),dim=1).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yqM_W0MmZsA_"
   },
   "outputs": [],
   "source": [
    "class SONN(nn.Module):\n",
    "    def __init__(self, address_size, ema_time_period, learning_rate_update, temperature, normalize=False):\n",
    "        super(SONN, self).__init__()\n",
    "        self.address_size = address_size\n",
    "        self.addresses = torch.zeros(1, address_size).to(device)\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ema = 0\n",
    "        self.ema_time_period = ema_time_period\n",
    "        self.ema_temperature = 2 / (self.ema_time_period + 1)\n",
    "        \n",
    "        self.learning_rate_update = learning_rate_update\n",
    "\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ema = 0\n",
    "        self.addresses = torch.zeros(1, self.address_size).to(device)\n",
    "        \n",
    "    def retrieve(self, query_address):\n",
    "        with torch.no_grad():\n",
    "            retrieved_content = torch.tensor([]).to(device)\n",
    "\n",
    "            cos = torch.nn.CosineSimilarity()\n",
    "            # Calculate the cosine similarities.\n",
    "            if self.normalize: \n",
    "                similarities = cos(self.addresses.sgn(), query_address.sgn())\n",
    "            else:\n",
    "                similarities = cos(self.addresses, query_address)\n",
    "            # Cosine distance tensor\n",
    "            distances = 1 - similarities\n",
    "\n",
    "            # Calculate the softmin weights.\n",
    "            softmin_weights = F.softmin(distances/self.temperature, dim=-1)\n",
    "\n",
    "            # Weight the memory addresses with the softmin weights.\n",
    "            weighted_addresses = torch.matmul(softmin_weights, self.addresses.to(device)).view(-1)\n",
    "\n",
    "            # Pool the weighted memory addresses to create the output.\n",
    "            retrieved_content = torch.sum(weighted_addresses.view(1, -1), 0)\n",
    "\n",
    "        return retrieved_content   \n",
    "\n",
    "    \n",
    "    def save(self, query_address):\n",
    "        cos = torch.nn.CosineSimilarity()\n",
    "        # Calculate the cosine similarities.\n",
    "        if self.normalize: \n",
    "            similarities = cos(self.addresses.sgn(), query_address.sgn())\n",
    "        else:\n",
    "            similarities = cos(self.addresses, query_address)\n",
    "\n",
    "        # Calculate the cosine distances.\n",
    "        distances = 1 - similarities\n",
    "        # Get the minimum distance and the corresponding address index.  \n",
    "        min_distance = torch.min(distances, dim=0)[0].item()\n",
    "        \n",
    "        # Calculate EMA for current chunk.\n",
    "        self.ema += self.ema_temperature * (min_distance - self.ema)\n",
    "        \n",
    "        # Check if the minimum distance is bigger than the adaptive threshold.\n",
    "        if min_distance > self.ema: # If the minimum distance is bigger, create a new address.\n",
    "            # Add a new entry to the address matrix/tensor equal to the target address.\n",
    "            self.addresses = torch.cat((self.addresses, query_address.view(1, -1)))\n",
    "        else: # If the minimum distance is smaller or equal, update the memory addresses.\n",
    "            # Apply the softmin function to the distance tensor the get the softmin weights.\n",
    "            softmin_weights = F.softmin(distances/self.temperature, dim=-1)\n",
    "            # Update the memory address space.\n",
    "            self.addresses += self.learning_rate_update * torch.mul(softmin_weights.view(-1, 1), query_address - self.addresses)\n",
    "         \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atomic_HVs_from_tokens_and_add_them_to_cleanup(tokens: List[str]) -> None:\n",
    "    global cleanup, dim\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token has been encountered before by querying the cleanup memory.\n",
    "        entry = cleanup.get(token)\n",
    "        # If it hasn't, \n",
    "        if entry == None:\n",
    "            # Generate a random HV representation for the token.\n",
    "            atomic_HV = thd.MAPTensor.random(1, dim)[0]\n",
    "            # Add the HV to the cleanup memory.\n",
    "            cleanup[token] = atomic_HV\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def generate_chunk_representations_and_save_them_to_memory(memory, tokens, chunk_lengths=[], output=False):\n",
    "    # \"n\" represents the no. of tokens in the sentence, which is also the max. no. of tokens \n",
    "    # that can be grouped to form a chunk.\n",
    "    n = len(tokens)\n",
    "    chunk_lengths = np.array(chunk_lengths, dtype=int)\n",
    "\n",
    "    # Generate all possible chunks.\n",
    "    if len(chunk_lengths) == 0:\n",
    "        chunk_lengths = np.arange(1, n +  1)\n",
    "    else:\n",
    "        # Remove lengths which are bigger than the maximum chunk length.\n",
    "        chunk_lengths = chunk_lengths[chunk_lengths <= n]\n",
    "   \n",
    "    for no_tokens in chunk_lengths:\n",
    "        if output:\n",
    "            print(\"no. of tokens: \", no_tokens)\n",
    "        for i in range(n):\n",
    "            if output:\n",
    "                print(\"start index: \", i)\n",
    "            # If there are not enough tokens left to construct a chunk comprised of \"no_tokens\", break. \n",
    "            if i + no_tokens > len(tokens):\n",
    "                if output:\n",
    "                    print(\"Not enough tokens left.\")\n",
    "                break \n",
    "            HC_representation = thd.MAPTensor.empty(1, dim)[0]\n",
    "\n",
    "            # Construct HC representation.\n",
    "            for j in range(no_tokens):\n",
    "                if output:\n",
    "                    print(tokens[i + j])\n",
    "                HC_representation += cleanup[tokens[i + j]]\n",
    "\n",
    "            # Save the chunk HC representation to memory.\n",
    "            memory.save(HC_representation)\n",
    "\n",
    "    return\n",
    "\n",
    "def generate_query(tokens: list):\n",
    "  n = len(tokens)\n",
    "  HC_representation = thd.MAPTensor.empty(1, dim)\n",
    "\n",
    "  for i in range(n):\n",
    "    # The token hasn't been encountered before.\n",
    "    if cleanup.get(tokens[i]) == None:\n",
    "        # Generate an atomic HC for the unencountered token.\n",
    "        atomic_HC = thd.MAPTensor.random(1, dim)[0]\n",
    "        # Add the atomic HC to the cleanup memory.\n",
    "        cleanup[tokens[i]] = atomic_HC\n",
    "        # Add the atomic (i.e., superpose) HC to the chunk HC representation.\n",
    "        HC_representation += atomic_HC\n",
    "    # The token has been encountered before.\n",
    "    else:\n",
    "        HC_representation += cleanup[tokens[i]]\n",
    "\n",
    "    return HC_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: Pruning code copied of original DSDM.\n",
    "def prune(self):\n",
    "        N_pruning = self.N_prune  # Maximum no. of (address) nodes the memory can have. \n",
    "        n_class = self.M.size(1)\n",
    "        # If the maximum number of nodes has been reached, apply LOF\n",
    "        # to get normalcy scores.\n",
    "        if len(self.Address) > N_pruning:   \n",
    "            clf = LocalOutlierFactor(n_neighbors=min(len(self.Address), self.n_neighbors), contamination=self.contamination)\n",
    "            A = self.Address\n",
    "            M = self.M\n",
    "            y_pred = clf.fit_predict(A.cpu())\n",
    "            X_scores = clf.negative_outlier_factor_\n",
    "            x_scor = torch.tensor(X_scores)\n",
    "            \n",
    "            # \"Naive\" pruning mode.\n",
    "            if self.prune_mode == \"naive\":\n",
    "                if len(A) > N_pruning:\n",
    "                    prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                    val, ind = torch.topk(x_scor, prun_N_addr) \n",
    "                    idx_remove = [True] * len(A)\n",
    "                    for i in ind:\n",
    "                        idx_remove[i] = False\n",
    "                    self.M = self.M[idx_remove] # Delete content from address.\n",
    "                    self.Address = self.Address[idx_remove] # Delete address.\n",
    "                    \n",
    "            # \"Balance\" pruning mode.\n",
    "            # Idea: Prune from each class instead of the nodes with the highest densities.\n",
    "            if self.prune_mode == \"balance\":\n",
    "                prun_N_addr = len(A) - N_pruning  # No. of addresses that must be pruned out.\n",
    "                mean_addr = N_pruning // n_class  # Max. number of allowed nodes per class.\n",
    "                val, ind = torch.sort(x_scor, descending=True)\n",
    "\n",
    "                count = prun_N_addr\n",
    "                idx_remove = [True] * len(A)\n",
    "                idx = 0\n",
    "                arg_m = torch.argmax(M, axis=1)  # Get predicted class.\n",
    "                N_remaining = torch.bincount(arg_m)  # Count the frequency of each value, i.e., no. of predictions for each class.\n",
    "                while count != 0:\n",
    "                    idx +=1\n",
    "                    indice = ind[idx]\n",
    "                    if N_remaining[arg_m[indice]] > (N_pruning // n_class):\n",
    "                        N_remaining[arg_m[indice]] -= 1\n",
    "                        idx_remove[ind[idx]] = False\n",
    "                        count-=1\n",
    "                self.M = self.M[idx_remove]\n",
    "                self.Address = self.Address[idx_remove]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i6SYEsfrZDql",
    "outputId": "2b6f0306-5541-454b-88a1-3f292e768c23"
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "lines_raw = load_data('../data/data.txt')\n",
    "\n",
    "# Preprocess input. \n",
    "lines_tokens = []\n",
    "for line_raw in lines_raw:\n",
    "    # Account for empty lines.\n",
    "    if line_raw.rstrip():\n",
    "        lines_tokens.append(preprocess_text(line_raw))\n",
    "\n",
    "\n",
    "address_size = dim\n",
    "ema_time_period = 500  # No. of days in the EMA, i.e., maximum number of save operations to be performed.\n",
    "learning_rate_update = 0.004\n",
    "temperature = 2.3\n",
    "\n",
    "# Create DSDM instance.\n",
    "memory_unnormalized = SONN(address_size=address_size, ema_time_period=ema_time_period, learning_rate_update=learning_rate_update, temperature=temperature)\n",
    "memory_normalized = SONN(address_size=address_size, ema_time_period=ema_time_period, learning_rate_update=learning_rate_update, temperature=temperature, normalize=True)\n",
    "\n",
    "memories = {\"normalized\": memory_normalized, \"unnormalized\": memory_unnormalized}\n",
    "\n",
    "# Flush cleanup memory.\n",
    "cleanup = {}\n",
    "\n",
    "# Train memory.\n",
    "for sentence_tokens in lines_tokens:\n",
    "    generate_atomic_HVs_from_tokens_and_add_them_to_cleanup(sentence_tokens)\n",
    "    for _, memory in memories.items():\n",
    "        generate_chunk_representations_and_save_them_to_memory(memory, sentence_tokens, chunk_lengths=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c94b52fb294f6c9585afde2acff16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similarities_to_atomic_HVs(memory, sentence):\n",
    "    global sims_df\n",
    "    retrieved_content = memory.retrieve(generate_query(preprocess_text(sentence)))\n",
    "\n",
    "\n",
    "    for token, atomic_HC in cleanup.items():\n",
    "        sims_df = pd.concat([sims_df, pd.DataFrame([{'sentence': sentence,\n",
    "                                                     'token': token,\n",
    "                                                     'similarity': thd.cosine_similarity(atomic_HC, retrieved_content).item()}])])\n",
    "\n",
    "    return\n",
    "\n",
    "out1, out2 = widgets.Output(), widgets.Output()  # Output widgets\n",
    "\n",
    "for out, (memory_type, memory) in zip([out1, out2], memories.items()):\n",
    "    sims_df = pd.DataFrame(columns=['sentence', 'token', 'similarity'])\n",
    "    \n",
    "    with out:\n",
    "        display(md(f\"### <ins>{memory_type.capitalize()}</ins>\"))\n",
    "        get_similarities_to_atomic_HVs(memory, \"The red house.\")\n",
    "        get_similarities_to_atomic_HVs(memory, \"The house.\")\n",
    "        get_similarities_to_atomic_HVs(memory, \"House.\")\n",
    "\n",
    "        display(sims_df.sort_values(['sentence', 'similarity'], ascending=False).set_index(['sentence', 'token']))\n",
    "        \n",
    "widgets.HBox([out1, out2])  # Display outout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <ins>Normalized</ins>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constructed addresses/abstract concepts:  26\n",
      "the house red\n",
      "the red\n",
      "house red\n",
      "the red house\n",
      "the green\n",
      "house green\n",
      "green house the\n",
      "the blue\n",
      "house blue\n",
      "blue house the\n",
      "house red\n",
      "the yellow\n",
      "house yellow\n",
      "the yellow house\n",
      "the blue\n",
      "house blue\n",
      "the blue\n",
      "house blue\n",
      "the red\n",
      "red house\n",
      "the red\n",
      "red house\n",
      "the red\n",
      "red house\n",
      "the red\n",
      "red house\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### <ins>Unnormalized</ins>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constructed addresses/abstract concepts:  13\n",
      "red the house\n",
      "red the\n",
      "red house\n",
      "the red house\n",
      "the green\n",
      "house green\n",
      "green house the\n",
      "the blue\n",
      "house blue\n",
      "blue house the\n",
      "the yellow\n",
      "house yellow\n",
      "the yellow house\n"
     ]
    }
   ],
   "source": [
    "def get_most_similar_HVs(sims_df, delta_threshold=0.15):\n",
    "    # Sort values: This is needed since similarity_next makes sense only in the context of a sort df.\n",
    "    df = sims_df.sort_values('similarity', ascending=False).reset_index(drop=True).copy()\n",
    "    # Add column with the previous token's similarity.\n",
    "    df['previous_token_similarity'] = df['similarity'].shift(1).values\n",
    "    # Compute the differece between the similarities. \n",
    "    df['delta'] = df['previous_token_similarity'] - df['similarity']\n",
    "    # Set the NaN value of the delta to '0', since the first token doesn't have a previous token.\n",
    "    df['delta'] = df['delta'].fillna(0)\n",
    "    # Get index of the first element whose delta is bigger than delta_threshold.\n",
    "    # TODO: Consider - This might have the edge case of all the deltas decreasing by delta_threshold.\n",
    "    idx_cut_in = df[df['delta'] > delta_threshold].head(1).index[0]\n",
    "    \n",
    "    # Subdataframe with only the most similar tokens.\n",
    "    most_similar_tokens_df = df.head(idx_cut_in)\n",
    "    \n",
    "    # Get concept as a string.\n",
    "    print(\" \".join(most_similar_tokens_df['token']))\n",
    "    #display(df)\n",
    "    return \n",
    "    \n",
    "\n",
    "def print_memory_addresses(memory):\n",
    "    print(\"Number of constructed addresses/abstract concepts: \", len(memory.addresses))\n",
    "\n",
    "\n",
    "    for address in memory.addresses:\n",
    "        sims_df = pd.DataFrame(columns=['token', 'similarity'])\n",
    "        for key, item in cleanup.items():\n",
    "            sims_df = pd.concat([sims_df, pd.DataFrame([{'token': key, 'similarity': thd.cosine_similarity(item,  address).item()}])])\n",
    "    \n",
    "        get_most_similar_HVs(sims_df)       \n",
    "    #display(sims_df.sort_values('similarity', ascending=False).reset_index(drop=True))\n",
    "    return\n",
    "\n",
    "\n",
    "for memory_type, memory in memories.items():\n",
    "    display(md(f\"### <ins>{memory_type.capitalize()}</ins>\"))\n",
    "    print_memory_addresses(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
