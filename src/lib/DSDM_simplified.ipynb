{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NDwnkbc5n-8C"
   },
   "source": [
    "# Dynamic Sparse Distributed Memory\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bUoR69d-oFP6"
   },
   "source": [
    "This notebook implements the DSDM model presented in [Online Task-free Continual Learning with Dynamic Sparse Distributed Memory](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850721.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [],
   "source": [
    "from hashlib import sha256\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "6iuLthXgNKLP"
   },
   "outputs": [],
   "source": [
    "# TODO: Move to experiment notebook.\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Vector dimension. TODO: Why was it chosen this high? Cite papers where confusion is not possible after a certain value.\n",
    "dim = 2000 \n",
    "n = 100000\n",
    "# TODO: Might make more sense to be a field in DSDM.\n",
    "cleanup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "txy0zDE7Zk4K"
   },
   "outputs": [],
   "source": [
    "def fix_seed():\n",
    "    seed = 42\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def load_data(path, bs=0, shuffle=False):\n",
    "    \"\"\"Load data from file path. \"\"\"\n",
    "    text = pathlib.Path(path).read_text(encoding='utf-8')\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"targets\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.targets[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[self.sub_indeces[index]]\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(sample[1])\n",
    "            sample = (sample[0], target)\n",
    "        return sample\n",
    "    \n",
    "\n",
    "def compute_distances_gpu(X, Y):\n",
    "    return torch.sqrt(-2 * torch.mm(X, Y.T) +\n",
    "                    torch.sum(torch.pow(Y, 2), dim=1) +\n",
    "                    torch.sum(torch.pow(X, 2), dim=1).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "yqM_W0MmZsA_"
   },
   "outputs": [],
   "source": [
    "# Class that implements a self-organizing neural network which models a DSDM.\n",
    "class SONN(nn.Module):\n",
    "    def __init__(self, Time_period, n_mini_batch, n_class=10, n_feat=384):\n",
    "        super(SONN, self).__init__()\n",
    "        self.n_feat = n_feat\n",
    "        self.n_class=n_class\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2/(Time_period + 1)\n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.count = 0\n",
    "        self.T = 1\n",
    "        self.Address = torch.zeros(1, n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class)\n",
    "        self.p_norm = \"fro\"\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.Time_period_Temperature = self.ema\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        self.dataset_name = \"MNIST\"\n",
    "        \n",
    "        self.acc_after_each_task = []\n",
    "        self.acc_aft_all_task = []\n",
    "        self.stock_feat = torch.tensor([]).to(device)\n",
    "        self.forgetting = []\n",
    "        self.N_prune = 5000 # Pruning threshold\n",
    "        self.prune_mode = \"balance\"\n",
    "        self.n_neighbors = 20\n",
    "        self.contamination = \"auto\"\n",
    "        self.pruning = False \n",
    "        self.cum_acc_activ = False\n",
    "        self.batch_test = True\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def apply_param(self, T, pruning, N_prune, n_neighbors, Time_period_Temperature):\n",
    "        self.T = T\n",
    "        self.pruning = True\n",
    "        self.N_prune = N_prune\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.Time_period_Temperature = Time_period_Temperature\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ema = 2 / (self.Time_period + 1)\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.count = 0\n",
    "        self.Address = torch.zeros(1, self.n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class).to(device)\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        \n",
    "    def retrieve(self, query_address, batch_test=False):\n",
    "        \"\"\"TODO: Add description.\"\"\"\n",
    "      # No gradient will be computed.\n",
    "        with torch.no_grad():\n",
    "            retrieved_content = torch.tensor([]).to(device)\n",
    "            # Get prediction.\n",
    "            if batch_test:\n",
    "                pass\n",
    "                # Compute distance from inputs to address space.\n",
    "                #distance = compute_distances_gpu(inputs, self.Address)\n",
    "                # Calculate address weight based on distance.\n",
    "                #soft_norm = F.softmin(distance/self.T, dim=-1)\n",
    "                # Pool weighted (come from the distance) content to get prediction.\n",
    "                #pred = torch.matmul(soft_norm, self.M)\n",
    "            else:\n",
    "                difference = query_address - self.Address\n",
    "                norm = torch.norm(difference, p=self.p_norm, dim=-1)\n",
    "                soft_norm = F.softmin(norm/self.T, dim=-1)\n",
    "                soft_pred = torch.matmul(soft_norm, self.M.to(device)).view(-1)\n",
    "                retrieved_content = torch.sum(soft_pred.view(1, -1), 0)\n",
    "        return retrieved_content   \n",
    "    \n",
    "    def prune(self):\n",
    "        N_pruning = self.N_prune\n",
    "        n_class = self.M.size(1)\n",
    "        if len(self.Address) > N_pruning:\n",
    "            clf = LocalOutlierFactor(n_neighbors=min(len(self.Address), self.n_neighbors), contamination=self.contamination)\n",
    "            A = self.Address\n",
    "            M = self.M\n",
    "            y_pred = clf.fit_predict(A.cpu())\n",
    "            X_scores = clf.negative_outlier_factor_\n",
    "            x_scor = torch.tensor(X_scores)\n",
    "            # \"Naive\" pruning mode.\n",
    "            if self.prune_mode == \"naive\":\n",
    "                if len(A) > N_pruning:\n",
    "                    prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                    val, ind = torch.topk(x_scor, prun_N_addr) \n",
    "                    idx_remove = [True] * len(A)\n",
    "                    for i in ind:\n",
    "                        idx_remove[i] = False\n",
    "                    self.M = self.M[idx_remove] # Delete content from address.\n",
    "                    self.Address = self.Address[idx_remove] # Delete address.\n",
    "            # \"Balance\" pruning mode.\n",
    "            if self.prune_mode == \"balance\":\n",
    "                prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                mean_addr = N_pruning // n_class\n",
    "                val, ind = torch.sort(x_scor, descending=True)\n",
    "\n",
    "                count = prun_N_addr\n",
    "                idx_remove = [True] * len(A)\n",
    "                idx = 0\n",
    "                arg_m = torch.argmax(M, axis=1)\n",
    "                N_remaining = torch.bincount(arg_m)\n",
    "                while count != 0:\n",
    "                    idx +=1\n",
    "                    indice = ind[idx]\n",
    "                    if N_remaining[arg_m[indice]] > (N_pruning // n_class):\n",
    "                        N_remaining[arg_m[indice]] -= 1\n",
    "                        idx_remove[ind[idx]] = False\n",
    "                        count-=1\n",
    "                self.M = self.M[idx_remove]\n",
    "                self.Address = self.Address[idx_remove]\n",
    "        \n",
    "    def test(self, testloader):\n",
    "      \"\"\" Test batch-wise. \"\"\"\n",
    "      total = 0\n",
    "      correct = 0\n",
    "\n",
    "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "          targets = targets.type(torch.LongTensor).to(device)\n",
    "          inputs = inputs.to(device)\n",
    "          # Pass inputs through NN to get prediction.\n",
    "          outputs = self.forward(inputs)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          total += targets.size(0)\n",
    "          correct += (predicted == targets).sum().item()\n",
    "\n",
    "      accuracy = correct / total * 100\n",
    "      print(\"test accuracy {:.3f} %,  {:.3f} / {:.3f} \".format(accuracy, correct, total))\n",
    "      return accuracy\n",
    "    \n",
    "    def test_idx(self, test_dataset_10_way_split, idx_test):\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for idx in idx_test:\n",
    "                curr_correct = 0\n",
    "                curr_total = 0\n",
    "                for batch_idx, (inputs, targets) in enumerate(test_dataset_10_way_split[idx]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.type(torch.LongTensor).to(device)\n",
    "                    # Pass inputs through NN to get prediction.\n",
    "                    outputs = self.forward(inputs)\n",
    "                    _, predicted = torch.max(outputs,1)\n",
    "                    total += targets.size(0)\n",
    "                    corr = (predicted == targets).sum().item()\n",
    "                    curr_correct +=corr\n",
    "                    correct += corr\n",
    "                    curr_total += targets.size(0)\n",
    "            accuracy = correct / total * 100\n",
    "        return accuracy, curr_correct / curr_total * 100\n",
    "    \n",
    "    def save(self, target_address, target_content, coef_global_error):\n",
    "        \"\"\"Add an item (target_address, target_content) to memory.\"\"\"\n",
    "        address_difference = target_address - self.Address # Difference tensor\n",
    "        # Distance tensor, where the distance is the Frobenius norm of the difference. \n",
    "        norm = torch.norm(address_difference, p=self.p_norm, dim=-1)\n",
    "        # Will be later fed as input to the softmin layer.\n",
    "        soft_norm = norm\n",
    "        # Get the minimum distance and the corresponding address index.  \n",
    "        min_distance = torch.min(norm, dim=0)[0].item()\n",
    "        # Adjust parameter based on the minimum distance..\n",
    "        self.global_error += self.ema_Temperature * (min_distance - self.global_error)\n",
    "\n",
    "        # Check if the minimum distance is bigger than the adaptive threshold.\n",
    "        # If the minimum distance is bigger, then:\n",
    "        \n",
    "        if min_distance >= self.global_error * coef_global_error:\n",
    "            # Add a new entry to the address matrix/tensor equal to the target address.\n",
    "            self.Address = torch.cat((self.Address, target_address.view(1, -1)))\n",
    "            # Add a new entry to the content matrix/tensor equal to the target content.\n",
    "            self.M = torch.cat((self.M, target_content.view(1, -1)))\n",
    "        # If the minimum distance is not bigger, then:\n",
    "        else:\n",
    "            # Apply the softmin function to the distance tensor the get a list of weights, that can be interpretated as probabilities.\n",
    "            soft_norm = F.softmin(norm/self.T, dim=-1)\n",
    "            # Weight the address difference by the corresponding address softmin weight and modify all the existing addresses. \n",
    "            self.Address += self.ema * torch.mul(soft_norm.view(-1, 1), address_difference)\n",
    "            # Weight the content difference by the corresponding address softmin weight and modify all the existing content entries. \n",
    "            self.M += self.ema * torch.mul(soft_norm.view(-1, 1), (target_content - self.M))\n",
    "        \n",
    "        return\n",
    "\n",
    "    def generate_and_save_chunks(self, tokens, coef_global_error):\n",
    "        # TODO: Move to the experiment notebook once you've figured out how to make DSDM as modular as possible.\n",
    "        \"\"\"TODO: Add function description.\"\"\"\n",
    "        # Generate 1-token chunks.\n",
    "        for token in tokens:\n",
    "            # Check if the chunk has been encountered before by querying the cleanup memory.\n",
    "            entry = cleanup.get(token)\n",
    "            # If it has not, then:\n",
    "            if entry == None:\n",
    "                # Generate a random HRR HV representation for the token.\n",
    "                val = thd.HRRTensor.random(1, dim)[0]\n",
    "                # Add val, key, and token to store.\n",
    "                cleanup[token] = {'val': val, 'trans': token}\n",
    "            # If it has, then:\n",
    "            else:\n",
    "                val = entry['val']\n",
    "            # Add chunk to the DSDM in an autoassociative manner.\n",
    "            self.save(val, val, coef_global_error)\n",
    "\n",
    "        # \"n\" represents the no. of tokens in the sentence, which is also the max. no. of tokens \n",
    "        # that can be grouped to form a chunk.\n",
    "        n = len(tokens)\n",
    "\n",
    "        for no_tokens in range(n + 1)[2:]:\n",
    "          print(\"no. of tokens: \", no_tokens)\n",
    "          for i in range(n):\n",
    "            print(\"start index: \", i)\n",
    "            # If there are not enough tokens left to construct a chunk comprised of \"no_tokens\", break. \n",
    "            if i + no_tokens > len(tokens):\n",
    "              print(\"Not enough tokens left.\")\n",
    "              break \n",
    "            val = thd.HRRTensor.empty(1, dim)[0]\n",
    "            _ = \"\"\n",
    "            # Construct val.\n",
    "            for j in range(no_tokens):\n",
    "              print(tokens[i + j])\n",
    "              _ += tokens[i + j] \n",
    "              _ += \" \"\n",
    "              val += thd.permute(cleanup[tokens[i + j]]['val'], shifts=no_tokens - j - 1) # TODO: you need the original position in the sentence.\n",
    "            # Check to see if val has been encountered before or not.\n",
    "            store_key = sha256(''.join([str(elem) for elem in val.tolist()]).encode('utf-8')).hexdigest()\n",
    "            if cleanup.get(store_key) == None:\n",
    "              # Add values to the cleanup memory.\n",
    "              cleanup[store_key] = {'val': val, 'trans': _}\n",
    "\n",
    "           # Add the chunk representation to DSDM.\n",
    "          self.save(val, val, coef_global_error)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def train__test_n_way_split(self, train_set, test_set, coef_global_error=1, ema_global_error=None, save_feat=False):\n",
    "        \"\"\"TODO: Add description.\"\"\"\n",
    "        # Sentence processing train loop. \n",
    "        for sentence in train_set:\n",
    "            # Generate chunks from the sentence and add them to DSDM.\n",
    "            self.generate_and_save_chunks(sentence, coef_global_error)\n",
    "\n",
    "            # Prune memory.\n",
    "            #if self.pruning:\n",
    "            #    self.prune()\n",
    "\n",
    "            return \n",
    "        \n",
    "    def grid_search_spread_factor(self, Time_period, n_mini_batch, train_set, test_set, N_try=1, ema_global_error=\"same\", coef_global_error=1, random_ordering=False):\n",
    "        \"\"\"Search for the best spread factor.\"\"\"\n",
    "        # Instantiate array with a length equal to the number of trials.\n",
    "        N_address_use = torch.zeros(N_try)\n",
    "        self.forgetting = []\n",
    "        \n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2 / (Time_period + 1)\n",
    "\n",
    "        for idx_try in tqdm(range(N_try)):\n",
    "            # Reset DSDM parameters.\n",
    "            self.reset()\n",
    "\n",
    "            # Get train and test accuracy for current trial.\n",
    "            self.train__test_n_way_split(train_set,\n",
    "                                         test_set,\n",
    "                                         ema_global_error=ema_global_error,\n",
    "                                         coef_global_error=coef_global_error)\n",
    "            # Number of generated addresses.\n",
    "            N_address_use[idx_try] = self.M.size(0)\n",
    "\n",
    "            # Shuffle the data randomly for a new trail.\n",
    "            if random_ordering:\n",
    "                dataset_shuffle = list(zip(train_dataset_10_way_split, test_dataset_10_way_split))\n",
    "                random.shuffle(dataset_shuffle)\n",
    "                train_dataset_10_way_split, test_dataset_10_way_split = zip(*dataset_shuffle)\n",
    "          \n",
    "        return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i6SYEsfrZDql",
    "outputId": "2b6f0306-5541-454b-88a1-3f292e768c23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00, 17.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tokens:  2\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "start index:  3\n",
      "is\n",
      "big\n",
      "start index:  4\n",
      "Not enough tokens left.\n",
      "no. of tokens:  3\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  3\n",
      "Not enough tokens left.\n",
      "no. of tokens:  4\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  2\n",
      "Not enough tokens left.\n",
      "no. of tokens:  5\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  1\n",
      "Not enough tokens left.\n",
      "no. of tokens:  2\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "start index:  3\n",
      "is\n",
      "big\n",
      "start index:  4\n",
      "Not enough tokens left.\n",
      "no. of tokens:  3\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  3\n",
      "Not enough tokens left.\n",
      "no. of tokens:  4\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  2\n",
      "Not enough tokens left.\n",
      "no. of tokens:  5\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  1\n",
      "Not enough tokens left.\n",
      "no. of tokens:  2\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "start index:  3\n",
      "is\n",
      "big\n",
      "start index:  4\n",
      "Not enough tokens left.\n",
      "no. of tokens:  3\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  3\n",
      "Not enough tokens left.\n",
      "no. of tokens:  4\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  2\n",
      "Not enough tokens left.\n",
      "no. of tokens:  5\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  1\n",
      "Not enough tokens left.\n",
      "no. of tokens:  2\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "start index:  3\n",
      "is\n",
      "big\n",
      "start index:  4\n",
      "Not enough tokens left.\n",
      "no. of tokens:  3\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  3\n",
      "Not enough tokens left.\n",
      "no. of tokens:  4\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start index:  2\n",
      "Not enough tokens left.\n",
      "no. of tokens:  5\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  1\n",
      "Not enough tokens left.\n",
      "no. of tokens:  2\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "start index:  3\n",
      "is\n",
      "big\n",
      "start index:  4\n",
      "Not enough tokens left.\n",
      "no. of tokens:  3\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  2\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  3\n",
      "Not enough tokens left.\n",
      "no. of tokens:  4\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "start index:  1\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  2\n",
      "Not enough tokens left.\n",
      "no. of tokens:  5\n",
      "start index:  0\n",
      "the\n",
      "red\n",
      "house\n",
      "is\n",
      "big\n",
      "start index:  1\n",
      "Not enough tokens left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "lines_raw = load_data('../data/data.txt')\n",
    "\n",
    "# Preprocess input. \n",
    "lines = []\n",
    "for line_raw in lines_raw:\n",
    "    lines.append(preprocess_text(line_raw))\n",
    "\n",
    "nprune = [0] #TODO: [1000, 2000, 5000, 10000]\n",
    "for i in nprune:\n",
    "    N_try = 5 \n",
    "    n_mini_batch = 55 \n",
    "    alpha = 1\n",
    "    Time_period = 500\n",
    "    Time_period_temperature = 150\n",
    "\n",
    "    # Instantiate DSDM instance.\n",
    "    sonn = SONN(Time_period, n_mini_batch, dim, n_feat=dim)\n",
    "    sonn.n_neighbors = 1000\n",
    "    sonn.contamination = \"auto\"\n",
    "    sonn.p_norm = \"fro\"\n",
    "    sonn.T = 2.3\n",
    "    sonn.pruning = True\n",
    "    sonn.N_prune = i\n",
    "    sonn.cum_acc_activ = True\n",
    "    sonn.Time_period_Temperature = Time_period_temperature\n",
    "    \n",
    "    # Flush cleanup memory.\n",
    "    cleanup = {}\n",
    "    # Train and test DSDM.\n",
    "    sonn.grid_search_spread_factor(Time_period,\n",
    "                                   n_mini_batch,\n",
    "                                   lines,\n",
    "                                   lines,\n",
    "                                   N_try,\n",
    "                                   ema_global_error=\"diff\",\n",
    "                                   coef_global_error=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(tokens: list):\n",
    "  n = len(tokens)\n",
    "  val = thd.HRRTensor.empty(1, dim)\n",
    "\n",
    "  for i in range(n):\n",
    "    # The token hasn't been encountered before.\n",
    "    if cleanup.get(tokens[i]) == None:\n",
    "        # Generate a random value for the unencountered token.\n",
    "        val += thd.HRRTensor.permute(thd.HRRTensor.random(1, dim), shifts=n - i - 1)\n",
    "    # The token has been encountered before.\n",
    "    else:\n",
    "        val += thd.permute(cleanup[tokens[i]]['val'], shifts=n - i - 1)\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red house is big</td>\n",
       "      <td>0.881230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house is big</td>\n",
       "      <td>0.878236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the red house is big</td>\n",
       "      <td>0.834457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is big</td>\n",
       "      <td>0.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big</td>\n",
       "      <td>0.671697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>0.178868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house</td>\n",
       "      <td>0.159950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>0.140828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>0.131422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the red house</td>\n",
       "      <td>0.124279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red house</td>\n",
       "      <td>0.116782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the red</td>\n",
       "      <td>0.113030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red house is</td>\n",
       "      <td>0.100969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the red house is</td>\n",
       "      <td>0.099681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house is</td>\n",
       "      <td>0.098929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   chunk       sim\n",
       "0      red house is big   0.881230\n",
       "0          house is big   0.878236\n",
       "0  the red house is big   0.834457\n",
       "0                is big   0.830100\n",
       "0                    big  0.671697\n",
       "0                    the  0.178868\n",
       "0                  house  0.159950\n",
       "0                     is  0.140828\n",
       "0                    red  0.131422\n",
       "0         the red house   0.124279\n",
       "0             red house   0.116782\n",
       "0               the red   0.113030\n",
       "0          red house is   0.100969\n",
       "0      the red house is   0.099681\n",
       "0              house is   0.098929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_content = sonn.retrieve(generate_query(preprocess_text(\"She likes.\")))\n",
    "\n",
    "sims_df = pd.DataFrame(columns=['chunk', 'sim'])\n",
    "\n",
    "for key, item in cleanup.items():\n",
    "  sims_df = pd.concat([sims_df, pd.DataFrame([{'chunk': cleanup[key]['trans'], 'sim': thd.cosine_similarity(cleanup[key]['val'],  retrieved_content).item()}])])\n",
    "\n",
    "display(sims_df.sort_values('sim', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
