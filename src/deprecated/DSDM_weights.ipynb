{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDwnkbc5n-8C"
   },
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set path for imports. ###\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory.\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\"))\n",
    "\n",
    "# Add the parent directory to the system path to be able to import modules from 'lib.'\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfs/home/dfichiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from hashlib import sha256\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from lib.utils import utils\n",
    "from lib.utils.preprocess import preprocess_text\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6iuLthXgNKLP"
   },
   "outputs": [],
   "source": [
    "# TODO: Move to experiment notebook.\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Vector dimension. TODO: Why was it chosen this high? Cite papers where confusion is not possible after a certain value.\n",
    "dim = 2000 \n",
    "n = 100000\n",
    "# TODO: Might make more sense to be a field in DSDM.\n",
    "cleanup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "txy0zDE7Zk4K"
   },
   "outputs": [],
   "source": [
    "def fix_seed():\n",
    "    seed = 42\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def load_data(path, bs=0, shuffle=False):\n",
    "    \"\"\"Load data from file path. \"\"\"\n",
    "    text = pathlib.Path(path).read_text(encoding='utf-8')\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"targets\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.targets[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[self.sub_indeces[index]]\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(sample[1])\n",
    "            sample = (sample[0], target)\n",
    "        return sample\n",
    "    \n",
    "\n",
    "def compute_distances_gpu(X, Y):\n",
    "    cos = torch.nn.CosineSimilarity()\n",
    "    return cos(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yqM_W0MmZsA_"
   },
   "outputs": [],
   "source": [
    "# Class that implements a self-organizing neural network which models a DSDM.\n",
    "class SONN(nn.Module):\n",
    "    def __init__(self, Time_period, n_mini_batch, n_class=10, n_feat=384):\n",
    "        super(SONN, self).__init__()\n",
    "        self.n_feat = n_feat\n",
    "        self.n_class=n_class\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2/(Time_period + 1)\n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.count = 0\n",
    "        self.T = 1\n",
    "        self.Address = torch.zeros(1, n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class)\n",
    "        self.p_norm = \"fro\"\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.Time_period_Temperature = self.ema\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        self.dataset_name = \"MNIST\"\n",
    "        \n",
    "        self.acc_after_each_task = []\n",
    "        self.acc_aft_all_task = []\n",
    "        self.stock_feat = torch.tensor([]).to(device)\n",
    "        self.forgetting = []\n",
    "        self.N_prune = 5000 # Pruning threshold\n",
    "        self.prune_mode = \"balance\"\n",
    "        self.n_neighbors = 20\n",
    "        self.contamination = \"auto\"\n",
    "        self.pruning = False \n",
    "        self.cum_acc_activ = False\n",
    "        self.batch_test = True\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def apply_param(self, T, pruning, N_prune, n_neighbors, Time_period_Temperature):\n",
    "        self.T = T\n",
    "        self.pruning = True\n",
    "        self.N_prune = N_prune\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.Time_period_Temperature = Time_period_Temperature\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ema = 2 / (self.Time_period + 1)\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.count = 0\n",
    "        self.Address = torch.zeros(1, self.n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class).to(device)\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        \n",
    "    def retrieve(self, query_address, batch_test=False):\n",
    "        \"\"\"TODO: Add description.\"\"\"\n",
    "      # No gradient will be computed.\n",
    "        with torch.no_grad():\n",
    "            retrieved_content = torch.tensor([]).to(device)\n",
    "            # Get prediction.\n",
    "            if batch_test:\n",
    "                pass\n",
    "                # Compute distance from inputs to address space.\n",
    "                #distance = compute_distances_gpu(inputs, self.Address)\n",
    "                # Calculate address weight based on distance.\n",
    "                #soft_norm = F.softmin(distance/self.T, dim=-1)\n",
    "                # Pool weighted (come from the distance) content to get prediction.\n",
    "                #pred = torch.matmul(soft_norm, self.M)\n",
    "            else:\n",
    "                cos = torch.nn.CosineSimilarity()\n",
    "                # Cosine similarity tensor \n",
    "                similarities = cos(self.Address, query_address)\n",
    "                # Cosine distance tensor\n",
    "                distances = 1 - similarities\n",
    "                soft_norm = F.softmin(distances/self.T, dim=-1)\n",
    "                soft_pred = torch.matmul(soft_norm, self.Address.to(device)).view(-1)\n",
    "                retrieved_content = torch.sum(soft_pred.view(1, -1), 0)\n",
    "        return retrieved_content   \n",
    "    \n",
    "    def prune(self):\n",
    "        N_pruning = self.N_prune\n",
    "        n_class = self.M.size(1)\n",
    "        if len(self.Address) > N_pruning:\n",
    "            clf = LocalOutlierFactor(n_neighbors=min(len(self.Address), self.n_neighbors), contamination=self.contamination)\n",
    "            A = self.Address\n",
    "            M = self.M\n",
    "            y_pred = clf.fit_predict(A.cpu())\n",
    "            X_scores = clf.negative_outlier_factor_\n",
    "            x_scor = torch.tensor(X_scores)\n",
    "            # \"Naive\" pruning mode.\n",
    "            if self.prune_mode == \"naive\":\n",
    "                if len(A) > N_pruning:\n",
    "                    prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                    val, ind = torch.topk(x_scor, prun_N_addr) \n",
    "                    idx_remove = [True] * len(A)\n",
    "                    for i in ind:\n",
    "                        idx_remove[i] = False\n",
    "                    self.M = self.M[idx_remove] # Delete content from address.\n",
    "                    self.Address = self.Address[idx_remove] # Delete address.\n",
    "            # \"Balance\" pruning mode.\n",
    "            if self.prune_mode == \"balance\":\n",
    "                prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                mean_addr = N_pruning // n_class\n",
    "                val, ind = torch.sort(x_scor, descending=True)\n",
    "\n",
    "                count = prun_N_addr\n",
    "                idx_remove = [True] * len(A)\n",
    "                idx = 0\n",
    "                arg_m = torch.argmax(M, axis=1)\n",
    "                N_remaining = torch.bincount(arg_m)\n",
    "                while count != 0:\n",
    "                    idx +=1\n",
    "                    indice = ind[idx]\n",
    "                    if N_remaining[arg_m[indice]] > (N_pruning // n_class):\n",
    "                        N_remaining[arg_m[indice]] -= 1\n",
    "                        idx_remove[ind[idx]] = False\n",
    "                        count-=1\n",
    "                self.M = self.M[idx_remove]\n",
    "                self.Address = self.Address[idx_remove]\n",
    "        \n",
    "    def test(self, testloader):\n",
    "      \"\"\" Test batch-wise. \"\"\"\n",
    "      total = 0\n",
    "      correct = 0\n",
    "\n",
    "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "          targets = targets.type(torch.LongTensor).to(device)\n",
    "          inputs = inputs.to(device)\n",
    "          # Pass inputs through NN to get prediction.\n",
    "          outputs = self.forward(inputs)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          total += targets.size(0)\n",
    "          correct += (predicted == targets).sum().item()\n",
    "\n",
    "      accuracy = correct / total * 100\n",
    "      print(\"test accuracy {:.3f} %,  {:.3f} / {:.3f} \".format(accuracy, correct, total))\n",
    "      return accuracy\n",
    "    \n",
    "    def test_idx(self, test_dataset_10_way_split, idx_test):\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for idx in idx_test:\n",
    "                curr_correct = 0\n",
    "                curr_total = 0\n",
    "                for batch_idx, (inputs, targets) in enumerate(test_dataset_10_way_split[idx]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.type(torch.LongTensor).to(device)\n",
    "                    # Pass inputs through NN to get prediction.\n",
    "                    outputs = self.forward(inputs)\n",
    "                    _, predicted = torch.max(outputs,1)\n",
    "                    total += targets.size(0)\n",
    "                    corr = (predicted == targets).sum().item()\n",
    "                    curr_correct +=corr\n",
    "                    correct += corr\n",
    "                    curr_total += targets.size(0)\n",
    "            accuracy = correct / total * 100\n",
    "        return accuracy, curr_correct / curr_total * 100\n",
    "    \n",
    "    def save(self, query_address, coef_global_error):\n",
    "        cos = torch.nn.CosineSimilarity()\n",
    "        # Cosine similarity tensor \n",
    "        similarities = cos(self.Address, query_address)\n",
    "        # Cosine distance tensor\n",
    "        distances = 1 - similarities\n",
    "        # Get the minimum distance and the corresponding address index.  \n",
    "        min_distance = torch.min(distances, dim=0)[0].item()\n",
    "        # TODO: Adjust parameter based on the minimum distance..\n",
    "        self.global_error += self.ema_Temperature * (min_distance - self.global_error)\n",
    "\n",
    "        # Check if the minimum distance is bigger than the adaptive threshold.\n",
    "        # If the minimum distance is bigger, create a new address.\n",
    "        if min_distance >= self.global_error * coef_global_error:\n",
    "            # Add a new entry to the address matrix/tensor equal to the target address.\n",
    "            self.Address = torch.cat((self.Address, query_address.view(1, -1)))\n",
    "        # If the minimum distance is not bigger, update  the existing addresses.\n",
    "        else:\n",
    "            # Apply the softmin function to the distance tensor the get the softmin weights.\n",
    "            soft_norm = F.softmin(distances/self.T, dim=-1)\n",
    "            # Update the addresses with the weighted query address.\n",
    "            print(self.ema * soft_norm) \n",
    "            self.Address += self.ema * torch.mul(soft_norm.view(-1, 1), query_address)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def generate_and_save_chunks(self, tokens, coef_global_error):\n",
    "        # TODO: Move to the experiment notebook once you've figured out how to make DSDM as modular as possible.\n",
    "        \"\"\"TODO: Add function description.\"\"\"\n",
    "        # Generate 1-token chunks.\n",
    "        for token in tokens:\n",
    "            # Check if the chunk has been encountered before by querying the cleanup memory.\n",
    "            entry = cleanup.get(token)\n",
    "            # If it has not, then:\n",
    "            if entry == None:\n",
    "                # Generate a random HRR HV representation for the token.\n",
    "                val = thd.HRRTensor.random(1, dim)[0]\n",
    "                # Add val, key, and token to store.\n",
    "                cleanup[token] = {'val': val, 'trans': token}\n",
    "            # If it has, then:\n",
    "            else:\n",
    "                val = entry['val']\n",
    "            # Add chunk to the DSDM in an autoassociative manner.\n",
    "            self.save(val, coef_global_error)\n",
    "\n",
    "        # \"n\" represents the no. of tokens in the sentence, which is also the max. no. of tokens \n",
    "        # that can be grouped to form a chunk.\n",
    "        n = len(tokens)\n",
    "\n",
    "        for no_tokens in range(n + 1)[2:]:\n",
    "          print(\"no. of tokens: \", no_tokens)\n",
    "          for i in range(n):\n",
    "            print(\"start index: \", i)\n",
    "            # If there are not enough tokens left to construct a chunk comprised of \"no_tokens\", break. \n",
    "            if i + no_tokens > len(tokens):\n",
    "              print(\"Not enough tokens left.\")\n",
    "              break \n",
    "            val = thd.HRRTensor.empty(1, dim)[0]\n",
    "            _ = \"\"\n",
    "            # Construct val.\n",
    "            for j in range(no_tokens):\n",
    "              print(tokens[i + j])\n",
    "              _ += tokens[i + j] \n",
    "              _ += \" \"\n",
    "              val += thd.permute(cleanup[tokens[i + j]]['val'], shifts=no_tokens - j - 1) # TODO: you need the original position in the sentence.\n",
    "            # Check to see if val has been encountered before or not.\n",
    "            store_key = sha256(''.join([str(elem) for elem in val.tolist()]).encode('utf-8')).hexdigest()\n",
    "            if cleanup.get(store_key) == None:\n",
    "              # Add values to the cleanup memory.\n",
    "              cleanup[store_key] = {'val': val, 'trans': _}\n",
    "\n",
    "           # Add the chunk representation to DSDM.\n",
    "          self.save(val, coef_global_error)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def train__test_n_way_split(self, train_set, test_set, coef_global_error=1, ema_global_error=None, save_feat=False):\n",
    "        \"\"\"TODO: Add description.\"\"\"\n",
    "        # Sentence processing train loop. \n",
    "        for sentence in train_set:\n",
    "            # Generate chunks from the sentence and add them to DSDM.\n",
    "            self.generate_and_save_chunks(sentence, coef_global_error)\n",
    "\n",
    "            # Prune memory.\n",
    "            #if self.pruning:\n",
    "            #    self.prune()\n",
    "\n",
    "        return \n",
    "        \n",
    "    def grid_search_spread_factor(self, Time_period, n_mini_batch, train_set, test_set, N_try=1, ema_global_error=\"same\", coef_global_error=1, random_ordering=False):\n",
    "        \"\"\"Search for the best spread factor.\"\"\"\n",
    "        # Instantiate array with a length equal to the number of trials.\n",
    "        N_address_use = torch.zeros(N_try)\n",
    "        self.forgetting = []\n",
    "        \n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2 / (Time_period + 1)\n",
    "\n",
    "        for idx_try in tqdm(range(N_try)):\n",
    "            # Reset DSDM parameters.\n",
    "            self.reset()\n",
    "\n",
    "            # Get train and test accuracy for current trial.\n",
    "            self.train__test_n_way_split(train_set,\n",
    "                                         test_set,\n",
    "                                         ema_global_error=ema_global_error,\n",
    "                                         coef_global_error=coef_global_error)\n",
    "            # Number of generated addresses.\n",
    "            N_address_use[idx_try] = self.Address.size(0)\n",
    "\n",
    "            # Shuffle the data randomly for a new trail.\n",
    "            if random_ordering:\n",
    "                dataset_shuffle = list(zip(train_dataset_10_way_split, test_dataset_10_way_split))\n",
    "                random.shuffle(dataset_shuffle)\n",
    "                train_dataset_10_way_split, test_dataset_10_way_split = zip(*dataset_shuffle)\n",
    "          \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i6SYEsfrZDql",
    "outputId": "2b6f0306-5541-454b-88a1-3f292e768c23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train and test DSDM.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43msonn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search_spread_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTime_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mn_mini_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mN_try\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mema_global_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcoef_global_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 267\u001b[0m, in \u001b[0;36mSONN.grid_search_spread_factor\u001b[0;34m(self, Time_period, n_mini_batch, train_set, test_set, N_try, ema_global_error, coef_global_error, random_ordering)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Get train and test accuracy for current trial.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain__test_n_way_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mema_global_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_global_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcoef_global_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_global_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Number of generated addresses.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m N_address_use[idx_try] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAddress\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 244\u001b[0m, in \u001b[0;36mSONN.train__test_n_way_split\u001b[0;34m(self, train_set, test_set, coef_global_error, ema_global_error, save_feat)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Sentence processing train loop. \u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Generate chunks from the sentence and add them to DSDM.\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_and_save_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_global_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Prune memory.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m#if self.pruning:\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m#    self.prune()\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 195\u001b[0m, in \u001b[0;36mSONN.generate_and_save_chunks\u001b[0;34m(self, tokens, coef_global_error)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Generate 1-token chunks.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Check if the chunk has been encountered before by querying the cleanup memory.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# If it has not, then:\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entry \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# Generate a random HRR HV representation for the token.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "lines_raw = [\"The red house is big.\", \"The green house is small.\"]\n",
    "\n",
    "# Preprocess input. \n",
    "lines = []\n",
    "for line_raw in lines_raw:\n",
    "    lines.append(preprocess_text(line_raw))\n",
    "\n",
    "nprune = [0] #TODO: [1000, 2000, 5000, 10000]\n",
    "for i in nprune:\n",
    "    N_try = 5 \n",
    "    n_mini_batch = 55 \n",
    "    alpha = 1\n",
    "    Time_period = 500\n",
    "    Time_period_temperature = 150\n",
    "\n",
    "    # Instantiate DSDM instance.\n",
    "    sonn = SONN(Time_period, n_mini_batch, dim, n_feat=dim)\n",
    "    sonn.n_neighbors = 1000\n",
    "    sonn.contamination = \"auto\"\n",
    "    sonn.p_norm = \"fro\"\n",
    "    sonn.T = 2.3\n",
    "    sonn.pruning = True\n",
    "    sonn.N_prune = i\n",
    "    sonn.cum_acc_activ = True\n",
    "    sonn.Time_period_Temperature = Time_period_temperature\n",
    "    \n",
    "    # Flush cleanup memory.\n",
    "    cleanup = {}\n",
    "    # Train and test DSDM.\n",
    "    sonn.grid_search_spread_factor(Time_period,\n",
    "                                   n_mini_batch,\n",
    "                                   lines,\n",
    "                                   lines,\n",
    "                                   N_try,\n",
    "                                   ema_global_error=\"diff\",\n",
    "                                   coef_global_error=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(tokens: list):\n",
    "  n = len(tokens)\n",
    "  val = thd.HRRTensor.empty(1, dim)\n",
    "\n",
    "  for i in range(n):\n",
    "    # The token hasn't been encountered before.\n",
    "    if cleanup.get(tokens[i]) == None:\n",
    "        # Generate a random value for the unencountered token.\n",
    "        val += thd.HRRTensor.permute(thd.HRRTensor.random(1, dim), shifts=n - i - 1)\n",
    "    # The token has been encountered before.\n",
    "    else:\n",
    "        val += thd.permute(cleanup[tokens[i]]['val'], shifts=n - i - 1)\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_content = sonn.retrieve(generate_query(preprocess_text(\"The blue house.\")))\n",
    "\n",
    "sims_df = pd.DataFrame(columns=['chunk', 'sim'])\n",
    "\n",
    "for key, item in cleanup.items():\n",
    "  sims_df = pd.concat([sims_df, pd.DataFrame([{'chunk': cleanup[key]['trans'], 'sim': thd.cosine_similarity(cleanup[key]['val'],  retrieved_content).item()}])])\n",
    "\n",
    "display(sims_df.sort_values('sim', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity()\n",
    "x = torch.Tensor([[1, 1], [1, 1]])\n",
    "y = torch.Tensor([1, 1])\n",
    "0.9 - cos(x,  y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why shouldn't we use positional vectors? Because the abstract concept is a mean of the most frequent tokens/representations. This means that we need to extract the tokens. We cannot presave the chunks because the order might be different, i.e., the token on the second position might disappear after normalization => We need to be able to extract the first and the last token only => They do not appear one after the other, are encoded with position => impossible. Let's see later if dropping the positional encoding has any grave consequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
