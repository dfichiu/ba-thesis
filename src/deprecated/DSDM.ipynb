{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDwnkbc5n-8C"
   },
   "source": [
    "# Dynamic Sparse Distributed Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUoR69d-oFP6"
   },
   "source": [
    "This notebook implements the DSDM model presented in [Online Task-free Continual Learning with Dynamic Sparse Distributed Memory](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850721.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dGeeI-H3NGMx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pathlib\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import torch\n",
    "import torchhd as thd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6iuLthXgNKLP"
   },
   "outputs": [],
   "source": [
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Vector dimension. TODO: Why was it chosen this high? Cite papers where confusion is not possible after a certain value.\n",
    "dim = 2000 \n",
    "n = 100000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This will be moved to the notebook where the experiments are made.\n",
    "cleanup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mTensor(lines[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ba-thesis-YAFM42rh-py3.10/lib/python3.10/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_raw = load_data('../data/data.txt')\n",
    "\n",
    "# List of hypevectorial sentence representations. \n",
    "lines = []\n",
    "for line_raw in lines_raw:\n",
    "    line = []\n",
    "\n",
    "    tokens = preprocess_text(line_raw)\n",
    "    for token in tokens:\n",
    "        # Check if the chunk has been encountered before.\n",
    "        val = cleanup.get(token)\n",
    "        # If it hasn't:\n",
    "        if val == None:\n",
    "            # Generate a random HRR HV representation for the token.\n",
    "            val = thd.random(1, dim, 'HRR')[0]\n",
    "            # Add representation to the cleanup memory.\n",
    "            cleanup[token] = {val}\n",
    "        line.append(val)\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "txy0zDE7Zk4K"
   },
   "outputs": [],
   "source": [
    "def fix_seed():\n",
    "    seed = 42\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def load_data(path, bs=0, shuffle=False):\n",
    "    \"\"\"Load data from file path. \"\"\"\n",
    "    text = pathlib.Path(path).read_text(encoding='utf-8')\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "\n",
    "    return lines\n",
    "    trainset = torch.utils.data.TensorDataset(lines, lines)\n",
    "    #testset = torch.utils.data.TensorDataset(loaded['testdata'], loaded['label_test'])\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                              batch_size=bs,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset,\n",
    "    #                                         batch_size=bs,\n",
    "    #                                         shuffle=shuffle,\n",
    "    #                                         num_workers=0)\n",
    "    return trainset,trainloader\n",
    "\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"targets\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.targets[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_indeces)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[self.sub_indeces[index]]\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(sample[1])\n",
    "            sample = (sample[0], target)\n",
    "        return sample\n",
    "    \n",
    "\n",
    "def compute_distances_gpu(X,Y):\n",
    "    return torch.sqrt(-2 * torch.mm(X,Y.T) +\n",
    "                    torch.sum(torch.pow(Y, 2),dim=1) +\n",
    "                    torch.sum(torch.pow(X, 2),dim=1).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yqM_W0MmZsA_"
   },
   "outputs": [],
   "source": [
    "# Class that implements a self-organizing neural network which models a DSDM.\n",
    "class SONN(nn.Module):\n",
    "    def __init__(self, Time_period, n_mini_batch, n_class=10, n_feat=384):\n",
    "        super(SONN, self).__init__()\n",
    "        self.n_feat = n_feat\n",
    "        self.n_class=n_class\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2/(Time_period + 1)\n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.count = 0\n",
    "        self.T = 1\n",
    "        self.Address = torch.zeros(1, n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class)\n",
    "        self.p_norm = \"fro\"\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.Time_period_Temperature = self.ema\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        self.dataset_name = \"MNIST\"\n",
    "        \n",
    "        self.acc_after_each_task = []\n",
    "        self.acc_aft_all_task = []\n",
    "        self.stock_feat = torch.tensor([]).to(device)\n",
    "        self.forgetting = []\n",
    "        self.N_prune = 5000 # Pruning threshold\n",
    "        self.prune_mode = \"balance\"\n",
    "        self.n_neighbors = 20\n",
    "        self.contamination = \"auto\"\n",
    "        self.pruning = False \n",
    "        self.cum_acc_activ = False\n",
    "        self.batch_test = True\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def apply_param(self, T, pruning, N_prune, n_neighbors, Time_period_Temperature):\n",
    "        self.T = T\n",
    "        self.pruning = True\n",
    "        self.N_prune = N_prune\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.Time_period_Temperature = Time_period_Temperature\n",
    "        \n",
    "    def reset(self):\n",
    "        self.ema = 2 / (self.Time_period + 1)\n",
    "        self.ema_Temperature = (2 / (self.Time_period_Temperature + 1))\n",
    "        self.count = 0\n",
    "        self.Address = torch.zeros(1, self.n_feat).to(device)\n",
    "        self.M = torch.zeros(1, self.n_class).to(device)\n",
    "        self.Error = torch.zeros(len(self.Address)).to(device)\n",
    "        self.global_error = 0\n",
    "        self.memory_global_error = torch.zeros(1)\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        self.memory_count_address = torch.zeros(1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "      # No gradient will be computed.\n",
    "        with torch.no_grad():\n",
    "            out = inputs\n",
    "            pred = torch.tensor([]).to(device)\n",
    "            # Get prediction.\n",
    "            if self.batch_test:\n",
    "                # Compute distance from inputs to adress space.\n",
    "                distance = compute_distances_gpu(inputs, self.Address)\n",
    "                # Calculate neuron weight based on distance.\n",
    "                soft_norm = F.softmin(distance/self.T, dim=-1)\n",
    "                # Pool weighted (come from the dtistance) content to get prediction.\n",
    "                pred = torch.matmul(soft_norm, self.M)\n",
    "            else:\n",
    "                for idx_x in range(len(out)):\n",
    "                    x = out[idx_x]\n",
    "                    distance = x - self.Address\n",
    "                    norm = torch.norm(distance, p=self.p_norm, dim=-1)\n",
    "                    # softmin vs argmin\n",
    "                    soft_norm = F.softmin(norm/self.T, dim=-1)\n",
    "                    soft_pred = torch.matmul(soft_norm, self.M.to(device)).view(-1)\n",
    "                    pred = torch.cat((pred, soft_pred.view(1,-1)), 0)\n",
    "        return pred   \n",
    "    \n",
    " def prune(self):\n",
    "        N_pruning = self.N_prune  # Maximum no. of (address) nodes the memory can have. \n",
    "        n_class = self.M.size(1)\n",
    "        # If the maximum number of nodes has been reached, apply LOF\n",
    "        # to get normalcy scores.\n",
    "        if len(self.Address) > N_pruning:   \n",
    "            clf = LocalOutlierFactor(n_neighbors=min(len(self.Address), self.n_neighbors), contamination=self.contamination)\n",
    "            A = self.Address\n",
    "            M = self.M\n",
    "            y_pred = clf.fit_predict(A.cpu())\n",
    "            X_scores = clf.negative_outlier_factor_\n",
    "            x_scor = torch.tensor(X_scores)\n",
    "\n",
    "            # \"Naive\" pruning mode.\n",
    "            if self.prune_mode == \"naive\":\n",
    "                if len(A) > N_pruning:\n",
    "                    prun_N_addr = len(A) - N_pruning # No. of addresses that must be pruned out.\n",
    "                    val, ind = torch.topk(x_scor, prun_N_addr) \n",
    "                    idx_remove = [True] * len(A)\n",
    "                    for i in ind:\n",
    "                        idx_remove[i] = False\n",
    "                    self.M = self.M[idx_remove] # Delete content from address.\n",
    "                    self.Address = self.Address[idx_remove] # Delete address.\n",
    "\n",
    "            # \"Balance\" pruning mode.\n",
    "            # Idea: Prune from each class instead of the nodes with the highest densities.\n",
    "            if self.prune_mode == \"balance\":\n",
    "                prun_N_addr = len(A) - N_pruning  # No. of addresses that must be pruned out.\n",
    "                mean_addr = N_pruning // n_class  # Max. number of allowed nodes per class.\n",
    "                val, ind = torch.sort(x_scor, descending=True)\n",
    "\n",
    "                count = prun_N_addr\n",
    "                idx_remove = [True] * len(A)\n",
    "                idx = 0\n",
    "                arg_m = torch.argmax(M, axis=1)  # Get predicted class.\n",
    "                N_remaining = torch.bincount(arg_m)  # Count the frequency of each value, i.e., no. of predictions for each class.\n",
    "                while count != 0:\n",
    "                    idx +=1\n",
    "                    indice = ind[idx]\n",
    "                    if N_remaining[arg_m[indice]] > (N_pruning // n_class):\n",
    "                        N_remaining[arg_m[indice]] -= 1\n",
    "                        idx_remove[ind[idx]] = False\n",
    "                        count-=1\n",
    "                self.M = self.M[idx_remove]\n",
    "                self.Address = self.Address[idx_remove]\n",
    "        return\n",
    "        \n",
    "    def test(self, testloader):\n",
    "      \"\"\" Test batch-wise. \"\"\"\n",
    "      total = 0\n",
    "      correct = 0\n",
    "\n",
    "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "          targets = targets.type(torch.LongTensor).to(device)\n",
    "          inputs = inputs.to(device)\n",
    "          # Pass inputs through NN to get prediction.\n",
    "          outputs = self.forward(inputs)\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          total += targets.size(0)\n",
    "          correct += (predicted == targets).sum().item()\n",
    "\n",
    "      accuracy = correct / total * 100\n",
    "      print(\"test accuracy {:.3f} %,  {:.3f} / {:.3f} \".format(accuracy, correct, total))\n",
    "      return accuracy\n",
    "    \n",
    "    def test_idx(self, test_dataset_10_way_split, idx_test):\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for idx in idx_test:\n",
    "                curr_correct = 0\n",
    "                curr_total = 0\n",
    "                for batch_idx, (inputs, targets) in enumerate(test_dataset_10_way_split[idx]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.type(torch.LongTensor).to(device)\n",
    "                    # Pass inputs through NN to get prediction.\n",
    "                    outputs = self.forward(inputs)\n",
    "                    _, predicted = torch.max(outputs,1)\n",
    "                    total += targets.size(0)\n",
    "                    corr = (predicted == targets).sum().item()\n",
    "                    curr_correct +=corr\n",
    "                    correct += corr\n",
    "                    curr_total += targets.size(0)\n",
    "            accuracy = correct / total * 100\n",
    "        return accuracy, curr_correct / curr_total * 100\n",
    "    \n",
    "    def train__test_n_way_split(self, train_dataset_10_way_split, test_dataset_10_way_split, coef_global_error=1, ema_global_error=None, plot=True, save_feat=False): # n equal len train_dataset_10_way_split\n",
    "        acc_test = torch.zeros(len(train_dataset_10_way_split)) # Tensor to hold the accuracies for each train split.\n",
    "        acc_test_after_each_task_softmin = torch.zeros(len(train_dataset_10_way_split))\n",
    "        self.memory_min_distance = torch.zeros(1)\n",
    "        idx_seen = []\n",
    "        self.cum_acc = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx_loader in range(len(train_dataset_10_way_split)):\n",
    "              # Mark task as already encountered.\n",
    "                idx_seen.append(idx_loader)\n",
    "\n",
    "                # Task train loop: Iterate over the batches of each task.\n",
    "                for batch_idx, (inputs, targets) in enumerate(train_dataset_10_way_split[idx_loader]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.type(torch.LongTensor).to(device)\n",
    "                    if batch_idx == self.n_mini_batch:\n",
    "                        break\n",
    "                    if save_feat:\n",
    "                        self.stock_feat = torch.cat((self.stock_feat, inputs))\n",
    "                    # Sample loop: Iterate over all samples in the current batch.\n",
    "                    for idx_x in range(len(inputs)):\n",
    "                        out = inputs\n",
    "                        x = out[idx_x]\n",
    "                        self.count += 1\n",
    "                        # List of bit-wise difference vectors: 0 - elements are the same & {1, -1} - elements are different. \n",
    "                        distance = x - self.Address\n",
    "                        # List of Hamming distances\n",
    "                        norm = torch.norm(distance, p=self.p_norm, dim=-1)\n",
    "                        # Will be later fed as input to an softmin layer.\n",
    "                        soft_norm = norm\n",
    "                        # Get minimum Hamming distance and the corresponding neuron index.  \n",
    "                        min_value, idx_min = torch.min(norm, 0)\n",
    "                        # Adjust parameter.\n",
    "                        self.global_error += self.ema_Temperature * (min_value - self.global_error)\n",
    "\n",
    "                        # Check if the distance is bigger than the adaptive threshold.\n",
    "                        if abs(norm[idx_min]) >= self.global_error * coef_global_error:\n",
    "                            # Create a new node with the adress currently being processed.\n",
    "                            self.Address = torch.cat((self.Address, x.view(1, -1)))\n",
    "                            # Create classification content encoding.\n",
    "                            targets_one_hot = F.one_hot(targets[idx_x], num_classes=self.n_class).float()\n",
    "                            # Create a nre entry into the memory.\n",
    "                            self.M = torch.cat((self.M, targets_one_hot.view(1, -1)))\n",
    "                        else:\n",
    "                            delta_address = distance\n",
    "                            # List of Hamming distances mapped to probabilities.\n",
    "                            soft_norm = F.softmin(norm/self.T, dim=-1)\n",
    "                            # Modify all existing addresses. The modification is made according to the weight. \n",
    "                            self.Address = self.Address + self.ema * torch.mul(soft_norm.view(-1, 1), delta_address)\n",
    "                            # Get content encoding.\n",
    "                            targets_one_hot = F.one_hot(targets[idx_x], num_classes=self.n_class).float()\n",
    "                            # Modify content accordingly.\n",
    "                            self.M += self.ema * torch.mul(soft_norm.view(-1, 1), (targets_one_hot - self.M))\n",
    "                # Prune memory.\n",
    "                if self.pruning:\n",
    "                    self.prune()\n",
    "                # \n",
    "                if self.cum_acc_activ:\n",
    "                    acc, last_acc = self.test_idx(test_dataset_10_way_split, idx_seen)\n",
    "                    self.cum_acc.append(acc)\n",
    "                    acc_test_after_each_task_softmin[idx_loader] = last_acc\n",
    "            # Update after task accuracy.\n",
    "            self.acc_after_each_task = acc_test_after_each_task_softmin\n",
    "            acc_test_after_each_task_softmin = acc_test_after_each_task_softmin\n",
    "            \n",
    "            acc_test_after_all_task_softmin = torch.zeros(len(train_dataset_10_way_split))\n",
    "            acc_test = torch.zeros(len(train_dataset_10_way_split))\n",
    "\n",
    "            # Test after all tasks are learned.\n",
    "            for idx_loader in range(len(test_dataset_10_way_split)):\n",
    "                count_input = 0\n",
    "                correct = 0\n",
    "                correct_softmin = 0\n",
    "\n",
    "                if self.batch_test:\n",
    "                    acc, last_acc = self.test_idx(test_dataset_10_way_split, [idx_loader])\n",
    "                    acc_test[idx_loader] = 0 \n",
    "                    acc_test_after_all_task_softmin[idx_loader] = last_acc\n",
    "                else:\n",
    "                    for batch_idx, (inputs, targets) in enumerate(test_dataset_10_way_split[idx_loader]):\n",
    "                        out = inputs.to(device)\n",
    "                        targets = targets.type(torch.LongTensor)\n",
    "                        for i in range(len(out)):\n",
    "                            count_input += 1\n",
    "                            x = out[i]\n",
    "                            distance = x-self.Address\n",
    "                            norm = torch.norm(distance, p=self.p_norm, dim=-1)\n",
    "                            # softmin\n",
    "                            soft_norm = F.softmin(norm/self.T, dim=-1)\n",
    "                            soft_pred = torch.matmul(soft_norm, self.M.to(device)).view(-1)\n",
    "                            arg_soft_pred = torch.argmax(soft_pred)\n",
    "                            # argmin\n",
    "                            idx = torch.argmin(norm)\n",
    "                            pred = torch.argmax(self.M[idx])\n",
    "                            if pred == targets[i]:\n",
    "                                correct += 1\n",
    "                            if arg_soft_pred == targets[i]:\n",
    "                                correct_softmin += 1\n",
    "                    acc_test[idx_loader] = correct / count_input*100\n",
    "                    acc_test_after_all_task_softmin[idx_loader] = correct_softmin / count_input * 100\n",
    "\n",
    "                self.acc_aft_all_task = acc_test_after_all_task_softmin\n",
    "            if plot:\n",
    "                plt.figure(figsize=(15,10))\n",
    "                plt.plot(acc_test)\n",
    "                plt.plot(acc_test_just_after_learn_1_task)\n",
    "                plt.plot(acc_test_after_all_task_softmin)\n",
    "                plt.legend([\"after learn all task\",\"after learn each task\",\"after learn all task with softmin\"])\n",
    "                plt.title(\"accuracy on all task = {:.2f} %\".format(acc_test.mean().item())+\"same accuracy with softmin (with T= {:.2f}) =\".format(self.T)+\"{:.3f}\".format(acc_test_after_all_task_softmin.mean().item())+\" on 10 way split cifar 10,\\n Time_period = \"+ str(Time_period)+ \" number of data per class = \"+str(self.n_mini_batch*bs)+\" address use \"+str(self.M.size(0)))  \n",
    "                plt.ylabel(\"test accuracy %\")\n",
    "                plt.xlabel(\"task 0 to task 9\")\n",
    "                for i in range(0,self.n_class):\n",
    "                    plt.vlines(i,min(acc_test),100,colors='k', linestyles='dotted', label='end task 0')\n",
    "                plt.show()\n",
    "                plt.figure(figsize=(15,10))\n",
    "                n_taskss=(np.linspace(0,len(test_dataset_10_way_split),len(self.memory_global_error)))\n",
    "                plt.plot(n_taskss,self.memory_global_error)\n",
    "                n_taskss=(np.linspace(0,len(test_dataset_10_way_split),len(self.memory_min_distance)))\n",
    "                plt.plot(n_taskss,self.memory_min_distance, alpha=0.3)\n",
    "                n_taskss=(np.linspace(0,len(test_dataset_10_way_split),len(self.memory_count_address)))\n",
    "                plt.plot(n_taskss,self.memory_count_address/max(self.memory_count_address)*100)\n",
    "                for i in range(1,len(test_dataset_10_way_split)+1):\n",
    "                    plt.vlines(i,0,70,colors='k', linestyles='dotted', label='end task 0')\n",
    "                plt.show()\n",
    "            return acc_test_after_each_task_softmin, acc_test_after_all_task_softmin\n",
    "        \n",
    "    def grid_search_spread_factor(self, Time_period, n_mini_batch, train_dataset_10_way_split, test_dataset_10_way_split, N_try=5, ema_global_error=\"same\", coef_global_error=1, plot=True, random_ordering=True):\n",
    "        \"\"\"Search for best spread factor.\"\"\"\n",
    "        acc_test = torch.zeros(N_try, len(train_dataset_10_way_split))\n",
    "        accuracy_std_test = torch.zeros(len(train_dataset_10_way_split))\n",
    "        accuracy_mean_test = torch.zeros(len(train_dataset_10_way_split))\n",
    "        \n",
    "        cum_acc = torch.zeros(N_try, len(train_dataset_10_way_split))\n",
    "        \n",
    "        acc_test_softmin = torch.zeros(N_try,len(train_dataset_10_way_split))\n",
    "        accuracy_mean_test_softmin = torch.zeros(len(train_dataset_10_way_split))\n",
    "        accuracy_std_test_softmin = torch.zeros(len(train_dataset_10_way_split))\n",
    "\n",
    "        # Instantiate array with a length equal to the number of trials.\n",
    "        N_address_use = torch.zeros(N_try)\n",
    "        self.forgetting = []\n",
    "        \n",
    "        self.n_mini_batch = n_mini_batch\n",
    "        self.Time_period = Time_period \n",
    "        self.ema = 2 / (Time_period + 1)\n",
    "\n",
    "        for idx_try in tqdm(range(N_try)):\n",
    "            # Reset DSDM parameters.\n",
    "            self.reset()\n",
    "\n",
    "            # Get train and test accuracy for current trial.\n",
    "            Acc_test, Acc_test_softmin = self.train__test_n_way_split(train_dataset_10_way_split,\n",
    "                                                                      test_dataset_10_way_split,\n",
    "                                                                      ema_global_error=ema_global_error,\n",
    "                                                                      plot=False,\n",
    "                                                                      coef_global_error=coef_global_error)\n",
    "            if self.cum_acc_activ:\n",
    "                cum_acc[idx_try] = torch.tensor(self.cum_acc)\n",
    "            self.forgetting.append((self.acc_after_each_task * 100 - self.acc_aft_all_task).mean())\n",
    "            acc_test[idx_try] = Acc_test\n",
    "            acc_test_softmin[idx_try] = Acc_test_softmin\n",
    "            # Number of generated addresses.\n",
    "            N_address_use[idx_try] = self.M.size(0)\n",
    "\n",
    "            # Shuffle the data randomly for a new trail.\n",
    "            if random_ordering:\n",
    "                dataset_shuffle = list(zip(train_dataset_10_way_split, test_dataset_10_way_split))\n",
    "                random.shuffle(dataset_shuffle)\n",
    "                train_dataset_10_way_split, test_dataset_10_way_split = zip(*dataset_shuffle)\n",
    "\n",
    "        accuracy_mean_test = acc_test.mean(0)\n",
    "        accuracy_std_test = acc_test.std(0)\n",
    "        accuracy_mean_test_softmin = acc_test_softmin.mean(0)\n",
    "        acc_soft_mean = acc_test_softmin.mean(1)\n",
    "        accuracy_std_test_softmin = acc_test_softmin.std(0)\n",
    "\n",
    "        print(\"forgetting softmin inference = {:.1f} % ± {:.1f}\".format(np.mean(self.forgetting), np.std(self.forgetting)))\n",
    "\n",
    "        if plot:\n",
    "            plt.errorbar(np.linspace(1,len(accuracy_mean_test_softmin),len(accuracy_mean_test_softmin)),accuracy_mean_test_softmin,accuracy_std_test_softmin,linestyle='None', fmt='o',capsize = 5)\n",
    "            print(str(len(train_dataset_10_way_split))+\"-way split, MNIST test accuracy on all task = {:.1f} % ± {:.1f}, mean Address use =  {:.1f} \".format(acc_soft_mean.mean().item(), acc_soft_mean.std().item(),N_address_use.mean()))#\n",
    "            print([\"accuracy softmin T_softmin = {:.1f} \".format(self.T)])\n",
    "            plt.xlabel(\"task 1 to task \"+str(len(train_dataset_10_way_split)))\n",
    "            plt.ylabel(\"Test Accuracy\")\n",
    "            plt.ylim(bottom=40)\n",
    "            plt.savefig(\"DSDM MNIST.pdf\")\n",
    "            plt.show()\n",
    "          \n",
    "        return acc_soft_mean, N_address_use.mean(), acc_test, cum_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kKaxPP9LZbfZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Not needed right now.\n",
    "\n",
    "file_path= \"/content/Core50_resnet18_224.pt\" \n",
    "bs = 50 # 50 samples per batch\n",
    "\n",
    "# Load data.\n",
    "trainset, testset, trainloader, testloader = load_preencoded_data(model_name, dataset_name, img_size, file_path=file_path)\n",
    "\n",
    "# Split train set into train set and validation set\n",
    "train_set, val_set = torch.utils.data.random_split(trainset, [25000, len(trainset)-25000])\n",
    "\n",
    "\n",
    "train_cifar100_100_way_split = []\n",
    "val_cifar100_100_way_split = []\n",
    "test_cifar100_100_way_split = []\n",
    "i = 0 # Used to construct labels\n",
    "\n",
    "train_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(train_set, [i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9]),\n",
    "                                                                batch_size=bs,\n",
    "                                                                shuffle=True,\n",
    "                                                                num_workers=0))\n",
    "val_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(val_set,[i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9]),\n",
    "                                                              batch_size=bs,\n",
    "                                                              shuffle=False,\n",
    "                                                              num_workers=0))\n",
    "test_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(testset,[i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9]),\n",
    "                                                               batch_size=bs,\n",
    "                                                               shuffle=False,\n",
    "                                                               num_workers=0))\n",
    "for i in range(10, 50, 5):\n",
    "    train_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(train_set, [i, i+1, i+2, i+3, i+4]),\n",
    "                                                                    batch_size=bs,\n",
    "                                                                    shuffle=True,\n",
    "                                                                    num_workers=0))\n",
    "    val_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(val_set, [i, i+1, i+2, i+3, i+4]),\n",
    "                                                                  batch_size=bs,\n",
    "                                                                  shuffle=False,\n",
    "                                                                  num_workers=0))\n",
    "    test_cifar100_100_way_split.append(torch.utils.data.DataLoader(SubDataset(testset, [i, i+1, i+2, i+3, i+4]),\n",
    "                                                                   batch_size=bs,\n",
    "                                                                   shuffle=False,\n",
    "                                                                   num_workers=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i6SYEsfrZDql",
    "outputId": "2b6f0306-5541-454b-88a1-3f292e768c23"
   },
   "outputs": [],
   "source": [
    "nprune = 0 #TODO: [1000, 2000, 5000, 10000]\n",
    "for i in nprune:\n",
    "    N_try = 5 \n",
    "    n_mini_batch = 55\n",
    "    alpha = 1\n",
    "    Time_period = 500\n",
    "    Time_period_temperature = 150\n",
    "\n",
    "    # Instantiate DSDM instance.\n",
    "    sonn = SONN(Time_period, n_mini_batch, 50, n_feat=512)\n",
    "    sonn.n_neighbors = 1000\n",
    "    sonn.contamination = \"auto\"\n",
    "    sonn.p_norm = \"fro\"\n",
    "    sonn.T = 2.3\n",
    "    sonn.pruning = True\n",
    "    sonn.N_prune = i\n",
    "    sonn.cum_acc_activ = True\n",
    "    sonn.Time_period_Temperature = Time_period_temperature\n",
    "\n",
    "    # Train and test DSDM.\n",
    "    accuracy_c100_100w, N_address_use_c100_100w, acc_test_softmin, cum_sum = sonn.grid_search_spread_factor(Time_period,\n",
    "                                                                                                            n_mini_batch,\n",
    "                                                                                                            train_cifar100_100_way_split,\n",
    "                                                                                                            test_cifar100_100_way_split,\n",
    "                                                                                                            N_try,\n",
    "                                                                                                            ema_global_error=\"diff\",\n",
    "                                                                                                            coef_global_error=alpha)\n",
    "    metric = accuracy_c100_100w.mean()\n",
    "    print(cum_sum.mean())\n",
    "    print(metric, len(sonn.Address))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
